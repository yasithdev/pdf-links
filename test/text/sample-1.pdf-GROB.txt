<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Appearance-based Gaze Estimation With Deep Learning: A Review and Benchmark</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-04-26">26 Apr 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yihua</forename><surname>Cheng</surname></persName>
							<email>yihuac@buaa.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory of Virtual Reality Technology and Systems</orgName>
								<orgName type="institution" key="instit1">SCSE</orgName>
								<orgName type="institution" key="instit2">Beihang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haofei</forename><surname>Wang</surname></persName>
							<email>wanghf@pcl.ac.cn</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Peng Cheng Laboratory</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yiwei</forename><surname>Bao</surname></persName>
							<email>baoyiwei@buaa.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory of Virtual Reality Technology and Systems</orgName>
								<orgName type="institution" key="instit1">SCSE</orgName>
								<orgName type="institution" key="instit2">Beihang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Feng</forename><surname>Lu</surname></persName>
							<email>lufeng@buaa.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory of Virtual Reality Technology and Systems</orgName>
								<orgName type="institution" key="instit1">SCSE</orgName>
								<orgName type="institution" key="instit2">Beihang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Peng Cheng Laboratory</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Appearance-based Gaze Estimation With Deep Learning: A Review and Benchmark</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-04-26">26 Apr 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2104.12668v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-07-19T01:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>gaze estimation</term>
					<term>eye appearance</term>
					<term>deep learning</term>
					<term>review</term>
					<term>benchmark</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Gaze estimation reveals where a person is looking. It is an important clue for understanding human intention. The recent development of deep learning has revolutionized many computer vision tasks, the appearance-based gaze estimation is no exception. However, it lacks a guideline for designing deep learning algorithms for gaze estimation tasks. In this paper, we present a comprehensive review of the appearance-based gaze estimation methods with deep learning. We summarize the processing pipeline and discuss these methods from four perspectives: deep feature extraction, deep neural network architecture design, personal calibration as well as device and platform. Since the data pre-processing and post-processing methods are crucial for gaze estimation, we also survey face/eye detection method, data rectification method, 2D/3D gaze conversion method, and gaze origin conversion method. To fairly compare the performance of various gaze estimation approaches, we characterize all the publicly available gaze estimation datasets and collect the code of typical gaze estimation algorithms. We implement these codes and set up a benchmark of converting the results of different methods into the same evaluation metrics. This paper not only serves as a reference to develop deep learning-based gaze estimation methods but also a guideline for future gaze estimation research. Implemented methods and data processing codes are available at http://phi-ai.org/GazeHub.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Eye gaze is one of the most important non-verbal communication cues. It contains rich information of human intent that enables researchers to gain insights into human cognition <ref type="bibr" target="#b1">[1]</ref>, <ref type="bibr" target="#b2">[2]</ref> and behavior <ref type="bibr" target="#b3">[3]</ref>, <ref type="bibr" target="#b4">[4]</ref>. It is widely demanded by various applications, e.g., human-computer interaction <ref type="bibr" target="#b5">[5]</ref>, <ref type="bibr" target="#b6">[6]</ref>, <ref type="bibr" target="#b7">[7]</ref> and head-mounted devices <ref type="bibr" target="#b8">[8]</ref>, <ref type="bibr" target="#b9">[9]</ref>, <ref type="bibr" target="#b10">[10]</ref>. To enable such applications, accurate gaze estimation methods are critical.</p><p>Over the last decades, a plethora of gaze estimation methods has been proposed. These methods usually fall into three categories: the 3D eye model recovery-based method, the 2D eye feature regression-based method and the appearance-based method. 3D eye model recovery-based methods construct a geometric 3D eye model and estimates gaze directions based on the model. The 3D eye model is usually person-specific due to the diversity of human eyes. Therefore, these methods usually require personal calibration to recover person-specific parameters such as iris radius and kappa angle. The 3D eye model recovery-based methods usually achieve reasonable Fig. <ref type="figure">1</ref>. Deep learning based gaze estimation relies on simple devices and complex deep learning algorithms to estimate human gaze. It usually uses off-the-shelf cameras to capture facial appearance, and employs deep learning algorithms to regress gaze from the appearance. According to this pipeline, we survey current deep learning based gaze estimation methods from four perspectives: deep feature extraction, deep neural network architecture design, personal calibration as well as device and platform. accuracy while they require dedicated devices such as infrared cameras. The 2D eye feature regression-based methods usually keep the same requirement on devices as 3D eye model recovery-based methods. The methods directly use the detected geometric eye feature such as pupil center and glint to regress the point of gaze (PoG). They do not require geometric calibration for converting gaze directions into PoG.</p><p>Appearance-based methods do not require dedicated devices, instead, it uses on-the-shelf web cameras to capture human eye appearance and regress gaze from the appearance. Although the setup is simple, it usually requires the following components: 1) An effective feature extractor to extract gaze features from high-dimensional raw image data. Some feature extractors such as histograms of oriented gradients are used in the conventional method <ref type="bibr" target="#b14">[14]</ref>. However, it can not effectively extract high-level gaze features from images. 2) A robust regression function to learn the mappings from appearance to human gaze. It is non-trivial to map the high-dimensional eye appearance to the low-dimensional gaze. Many regression functions have been used to regress gaze from appearance, e.g., local linear interpolation <ref type="bibr" target="#b15">[15]</ref>, adaptive linear regression <ref type="bibr" target="#b13">[13]</ref> and gaussian process regression <ref type="bibr" target="#b16">[16]</ref> , the regression performance is barely satisfactory. 3) A large number of training samples to learn the regression function. They usually collects personal samples with a time-consuming personal calibration,   <ref type="bibr" target="#b11">[11]</ref> to off-shelf web cameras, gaze estimation becomes more convenient. (b) Gaze estimation methods are also updated with the change of devices. We illustrate five kinds of gaze estimation methods. <ref type="bibr" target="#b1">(1)</ref>. Attached sensor based methods. The method samples the electrical signal of skin electrodes. The signal indicates the eye movement of subjects <ref type="bibr" target="#b12">[12]</ref>. (2) 3D eye model recovery methods. The method usually builds a geometric eye model to calculate the visual axis, i.e., gaze directions. The eye model is fitted based on the light reflection. (3) 2D eye feature regression methods. The method relies on IR cameras to detect geometric eye features such as pupil center, glints, and directly regress the PoG from these features. (4) Conventional appearance-based methods. The method use entire images as feature and directly regress human gaze from features. Some feature reduction methods are also used for extracting low-dimensional feature. For example, Lu et al. divide eye images into 15 subregion and sum the pixel intensities in each subregion as feature <ref type="bibr" target="#b13">[13]</ref>. <ref type="bibr" target="#b5">(5)</ref> Appearance-based gaze estimation with deep learning. Face or eye images are directly inputted into a designed neural network to estimate human gaze. and learns a person-specific gaze estimation model. Some studies seek to reduce the number of training samples. <ref type="bibr">Lu et al.</ref> propose an adaptive linear regression method to select an optimal set of sparsest training sample for interpolation <ref type="bibr" target="#b13">[13]</ref>. However, this also limits the usage in real-world applications.</p><p>Recently, deep learning-based gaze estimation approaches have become a research hotspot. Compared with conventional appearance based methods, deep learning based methods demonstrate many advantages. 1) It can extract highlevel abstract gaze features from high-dimensional images.</p><p>2) It learns a highly non-linear mapping function from eye appearance to gaze. These advantages make deep learningbased methods are more robust and accurate than conventional methods. Conventional appearance-based methods often have performance drop when meet head motion, while deep learning-based methods tolerate head movement to some extent. Deep learning-based methods also improve the cross-subject gaze estimation performance with a large margin. These improvements greatly expand the application range of appearance-based gaze estimation methods.</p><p>In this paper, we provide a comprehensive review of appearance-based gaze estimation methods in deep learning. As shown in Fig. <ref type="figure">1</ref>, we discuss these methods from four perspectives: 1) deep feature extraction, 2) deep neural network architecture design, 3) personal calibration, 4) device and platform. In the deep feature extraction perspective, we describe how to extract effective feature in the current methods. We divide the raw feature into eye images, face images and videos. The algorithm for extracting high-level feature from the three raw features is respectively reviewed in this part. In the deep neural network architecture design perspective, we review advanced CNN models. According to the supervision mechanism, we respective review supervised, selfsupervised, semi-supervised and unsupervised gaze estimation methods. We also describe different CNN architectures in gaze estimation including multi-task CNNs and recurrent CNNs. In addition, some methods integrate CNN models and prior knowledges of gaze. These methods are also introduced in this part. In the personal calibration perspective, we describe how to use calibration samples to further improve the performance of CNNs. We also introduce the method integrating userunaware calibration sample collection mechanism. Finally, in the device and platforms perspective, we consider different cameras, i.e., RGB cameras, IR cameras and depth cameras, and different platforms, i.e., computer, mobile devices and head-mount device. We review the advanced methods using these cameras and proposed for these platforms.</p><p>Besides deep learning-based gaze estimation methods, we also focus on the practice of gaze estimation. We first review the data pre-processing methods of gaze estimation including face and eye detection methods, and common data rectification methods. Then, considering various forms of human gaze, e.g., gaze direction and PoG, we further provide data post-processing methods. The methods describe the geometric conversion between various human gaze. We also build gaze estimation benchmarks based on the data post-processing methods. We collect and implement the codes of typical gaze estimation methods, and evaluate them on various datasets. For the different kinds of gaze estimation methods, we convert their result for comparison with the data post-processing methods. The benchmark provides comprehensive and fair comparison between state-of-the-art gaze estimation methods.</p><p>The paper is organized as follows. Section II introduces the background of gaze estimation. We introduce the development and category of gaze estimation methods. Section III reviews the state-of-the-art deep learning based method. In Section IV, we introduce the public datasets as well as data pre-processing and post-processing methods. We also build the benchmark in this section. In Section V, we conclude the development of current deep learning-based methods and recommend future research directions. This paper can not only serve as a reference to develop deep learning based-gaze estimation methods, but also a guideline for future gaze estimation research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. GAZE ESTIMATION BACKGROUND</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Categorization</head><p>Gaze estimation research has a long history. Figure <ref type="figure" target="#fig_1">2</ref> illustrates the development progress of gaze estimation methods. Early gaze estimation methods rely on detecting eye movement patterns such as fixation, saccade and smooth pursuit <ref type="bibr" target="#b11">[11]</ref>. They attach the sensors around the eye and use potential differences to measure eye movement <ref type="bibr" target="#b17">[17]</ref>, <ref type="bibr" target="#b18">[18]</ref>. With the development of computer vision technology, modern eyetracking devices have emerged. These methods usually estimate gaze using the eye/face images captured by a camera. In general, there are two types of such devices, the remote eye tracker and the head-mounted eye tracker. The remote eye tracker usually keeps a certain distance from the user, typically ∼60 cm. The head-mounted eye tracker usually mounts the cameras on a frame of glasses. Compared to the intrusive eye tracking devices, the modern eye tracker greatly enlarges the range of application with computer vision-based methods.</p><p>Computer vision-based methods can be further divided into three types: the 2D eye feature regression method, the 3D eye model recovery method and the appearance-based method. The first two types of methods estimate gaze based on detecting geometric features such as contours, reflection and eye corners. The geometric features can be accurately extracted with the assistance of dedicated devices, e.g., infrared cameras. Detailly, the 2D eye feature regression method learns a mapping function from the geometric feature to the human gaze, e.g., the polynomials <ref type="bibr" target="#b19">[19]</ref>, <ref type="bibr" target="#b20">[20]</ref> and the neural networks <ref type="bibr" target="#b21">[21]</ref>. The 3D eye model recovery method builds a subject-specific geometric eye model to estimate the human gaze. The eye model is fitted with geometric features, such as the infrared corneal reflections <ref type="bibr" target="#b22">[22]</ref>, <ref type="bibr" target="#b23">[23]</ref>, pupil center <ref type="bibr" target="#b24">[24]</ref> and iris contours <ref type="bibr" target="#b25">[25]</ref>. In addition, the eye model contains subject-specific parameters such as cornea radius, kappa angles. Therefore, it usually requires time-consuming personal calibration to estimate these subject-specific parameters for each subject.</p><p>Appearance-based methods directly learn a mapping function from images to human gaze. Different from 2D eye feature regression methods, appearance-based methods do not require dedicated devices for detecting geometric features. They use image features such as image pixel <ref type="bibr" target="#b13">[13]</ref> or deep features <ref type="bibr" target="#b26">[26]</ref> to regress gaze. Various regression models have been used, e.g., the neural network <ref type="bibr" target="#b27">[27]</ref>, the Gaussian process regression model <ref type="bibr" target="#b16">[16]</ref>, the adaptive linear regression model <ref type="bibr" target="#b13">[13]</ref> and the convolutional neural network <ref type="bibr" target="#b26">[26]</ref>. However, this is still a challenging task due to the complex eye appearance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Appearance-based Gaze Estimation</head><p>Appearance-based methods directly learn the mapping function from eye appearance to human gaze. As early in 1994, Baluja et al. propose a neural network and collect 2,000 samples for training <ref type="bibr" target="#b27">[27]</ref>. Tan et al. use a linear function to interpolate unknown gaze position using 252 training samples <ref type="bibr" target="#b15">[15]</ref>. Early appearance-based methods usually learn a subject-specific mapping function. They require a timeconsuming calibration to collect the training samples of the specific subject. To reduce the number of training samples, Williams et al. introduce semi-supervised Gaussian process regression methods <ref type="bibr" target="#b16">[16]</ref>. Sugano et al. propose a method that combines gaze estimation with saliency <ref type="bibr" target="#b28">[28]</ref>. <ref type="bibr">Lu et al.</ref> propose an adaptive linear regression method to select an optimal set of sparsest training sample for interpolation <ref type="bibr" target="#b13">[13]</ref>. However, these methods only show reasonable performance in a constrained environment, i.e., fixed head pose and the specific subject. Their performance significantly degrades when tested on an unconstrained environment. This problem is always challenging in appearance-based gaze estimation.</p><p>To address the performance degradation problem across subjects, Funes et al. presented a cross-subject training method <ref type="bibr" target="#b29">[29]</ref>. However, the reported mean error is larger than 10 degrees. Sugano et al. introduce a learning-by-synthesis method <ref type="bibr" target="#b30">[30]</ref>. They use the large number of synthetic crosssubject data to train their model. Lu et al. employ a sparse auto-encoder to learn a set of bases from eye image patches and reconstruct the eye image using these bases <ref type="bibr" target="#b31">[31]</ref>. To  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Deep Learning for Appearance-based Gaze Estimation</head><p>Appearance-based gaze estimation suffers from many challenges, such as head motion and subject differences, especially in the unconstrained environment. These factors have a large impact on the eye appearance and complicate the eye appearance. Conventional appearance-based methods cannot handle these challenges gracefully due to the weak fitting ability.</p><p>Convolutional neural networks (CNNs) have been used in many computer vision tasks and demonstrate outstanding performance. Zhang et al. propose the first CNN-based gaze estimation method to regress gaze directions from eye images <ref type="bibr" target="#b26">[26]</ref>. They use a simple CNN and the performance surpasses most of the conventional appearance-based approaches. Following this study, an increasing number of improvements and extensions on CNN-based gaze estimation methods emerged. Face images <ref type="bibr" target="#b35">[35]</ref> and videos <ref type="bibr" target="#b36">[36]</ref> are used as input to the CNN for gaze estimation. These inputs provide more valuable information than using eye images alone. Some methods are proposed for handling the challenges in an unconstrained environment. For example, Cheng et al. use asymmetric regression to handle the extreme head pose and illumination condition <ref type="bibr" target="#b37">[37]</ref>. Park et al. learn a pictorial eye representation to alleviate the personal appearance difference <ref type="bibr" target="#b38">[38]</ref>. The calibration-based methods are also proposed to learn a subject-specific CNN model <ref type="bibr" target="#b39">[39]</ref>, <ref type="bibr" target="#b40">[40]</ref>. The vulnerability of appearance-based gaze estimation is also learned in <ref type="bibr" target="#b41">[41]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. DEEP GAZE ESTIMATION FROM APPEARANCE</head><p>We survey current deep learning based gaze estimation methods in this section. We introduce these methods from fours perspectives, deep feature extraction, deep neural network architecture design, personal calibration as well as device and platform. Figure <ref type="figure" target="#fig_3">3</ref> gives an overview of this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Deep Feature From Appearance</head><p>Feature extraction is critical in most of the learning-based tasks. Effectively extracting features from eye appearance is challenging due to the complex eye appearance. The quality of the extracted features determines the gaze estimation accuracy. Here, we summarize the feature extraction method according to the types of input into the deep neural network: eye images, face images and videos.</p><p>1) Feature from eye images: The gaze direction is highly correlated with the eye appearance. Any perturbation in gaze direction results in eye appearance changes. For example, the rotation of the eyeball changes the location of the iris and the shape of the eyelid, which leads to changes in gaze direction. This relationship makes it possible to estimate gaze from eye appearance. Conventional methods usually estimate gaze from high-dimensional raw image features. These features are directly generated from eye images by raster scanning all the pixels <ref type="bibr" target="#b15">[15]</ref>, <ref type="bibr" target="#b44">[44]</ref>. The features are highly redundant and can not handle environmental changes.</p><p>Deep learning-based methods automatically extract deep features from eye images. Zhang et al. proposed the first deep learning-based gaze estimation method <ref type="bibr" target="#b26">[26]</ref>. They employ a CNN to extract the features from grey-scale single eye images and concatenate these features with an estimated head pose. As with most deep learning tasks, the deeper network structure and larger receptive field, the more informative features can be extracted. In <ref type="bibr" target="#b42">[42]</ref>, Zhang et al. further extend their previous work <ref type="bibr" target="#b26">[26]</ref> and present a GazeNet which is a 13-convolutionallayer neural network inherited from a 16-layer VGG network <ref type="bibr" target="#b45">[45]</ref> as shown in Fig. <ref type="figure" target="#fig_5">4</ref> (a). They demonstrate that the GazeNet outperforms the LeNet-based approach presented in <ref type="bibr" target="#b26">[26]</ref>. Chen et al. <ref type="bibr" target="#b46">[46]</ref> use dilated convolutions to extract highlevel eye features, which efficiently increases the receptive field size of the convolutional filters without reducing spatial resolution.</p><p>Early deep learning-based methods estimate the gaze from single eye image. Recent studies found that concatenating the features of two eyes help to improve the gaze estimation accuracy <ref type="bibr" target="#b47">[47]</ref>, <ref type="bibr" target="#b48">[48]</ref>. Fischer et al. <ref type="bibr" target="#b47">[47]</ref> employ two VGG-16 networks <ref type="bibr" target="#b45">[45]</ref> to extract the individual features from two eye images, and concatenate two eye features for regression. Cheng et al. <ref type="bibr" target="#b48">[48]</ref> build a four-stream CNN network for extracting features from two eye images. Two streams of CNN are used for extracting individual features from left/right eye images, the other two streams are used for extracting joint features of two eye images. They claim that the two eyes are asymmetric. Thus, they propose an asymmetric regression and evaluation network to extract the different features from two eyes. However, the studies in <ref type="bibr" target="#b47">[47]</ref>, <ref type="bibr" target="#b48">[48]</ref> simply concatenate the left and right eye features to form new feature vectors, more   <ref type="bibr" target="#b42">[42]</ref>. (b) Gaze estimation with face images <ref type="bibr" target="#b43">[43]</ref>. (c). Gaze estimation with face and eye images <ref type="bibr" target="#b35">[35]</ref>.</p><p>recent studies propose to use attention mechanism for fusing two eye features. Cheng et al. <ref type="bibr" target="#b49">[49]</ref> argue that the weights of two eye features are determined by face images due to the specific task in <ref type="bibr" target="#b49">[49]</ref>, so they assign weights with the guidance of facial features. Bao et al. <ref type="bibr" target="#b50">[50]</ref> propose a self-attention mechanism to fuse two eye features. They concatenate the feature maps of two eyes and use a convolution layer to generate the weights of the feature map.</p><p>The above-mentioned methods extract the general features from eye images, some works explored extracting special features to handle the head motion and subject difference. Extracting subject-invariant gaze features has become a research hotspot. Eye appearance varies by much across different people. The ultimate solution is to collect training data that covers the whole data space, however, it is practically impossible. Several studies have attempted to extract subjectinvariant features from eye images <ref type="bibr" target="#b38">[38]</ref>, <ref type="bibr" target="#b51">[51]</ref>, <ref type="bibr" target="#b40">[40]</ref>. Park et al. <ref type="bibr" target="#b38">[38]</ref> convert the original eye images into a unified gaze representation, which is a pictorial representation of the eyeball, the iris and the pupil. They regress the gaze direction from the pictorial representation. Wang et al. propose an adversarial learning approach to extract the domain/personinvariant feature <ref type="bibr" target="#b51">[51]</ref>. They feed the features into an additional classifier and design an adversarial loss function to handle the appearance variations. Park et al. use an autoencoder to learn the compact latent representation of gaze, head pose and appearance <ref type="bibr" target="#b40">[40]</ref>. They introduce a geometric constraint on gaze representations, i.e., the rotation matrix between the two given images transforms the gaze representation of one image to another. In addition, some methods use GAN to pre-process eye images to handle some specific environment factors. Kim et al. <ref type="bibr" target="#b52">[52]</ref> utilize a GAN to convert low-light eye images into bright eye images. Rangesh et al. <ref type="bibr" target="#b53">[53]</ref> use a GAN to remove eyeglasses.</p><p>Besides the supervised approaches for extracting gaze features, unannotated eye images have also been used for learning gaze representations. Yu et al. propose to use the difference of gaze representations from two eyes as input to a gaze redirection network <ref type="bibr" target="#b54">[54]</ref>. They use the unannotated eye images to perform the unsupervised gaze representation learning.</p><p>2) Feature from face images: Face images contain the head pose information that also contributes to gaze estimation. Conventional methods have explored extracting features using face images. They usually extract features such as head pose <ref type="bibr" target="#b34">[34]</ref> and facial landmarks <ref type="bibr" target="#b55">[55]</ref>, <ref type="bibr" target="#b56">[56]</ref>, <ref type="bibr" target="#b57">[57]</ref>. The early eye imagebased method uses the estimated head pose as an additional input <ref type="bibr" target="#b26">[26]</ref>. However, the feature is proved to be useless for the deep learning-based method <ref type="bibr" target="#b42">[42]</ref>. Some studies directly use face images as input and employ a CNN to automatically extract deep facial features <ref type="bibr" target="#b43">[43]</ref>, <ref type="bibr" target="#b35">[35]</ref> as shown in Fig. <ref type="figure" target="#fig_5">4 (b)</ref>. It demonstrates an improved performance than the approaches that only use eye images.</p><p>Face images contain redundant information. Researchers have attempted to filter out the useless features in face image <ref type="bibr" target="#b43">[43]</ref>, <ref type="bibr" target="#b58">[58]</ref>. Zhang et al. <ref type="bibr" target="#b43">[43]</ref> propose a spatial weighting mechanism to efficiently encode the location of the face into a standard CNN architecture. The system learns spatial weights based on the activation maps of the convolutional layers. This helps to suppress the noise and enhance the contribution of the highly activated regions. Zhang et al. <ref type="bibr" target="#b59">[59]</ref> propose a learning-based region selection method. They dynamically select suitable sub-regions from facial region for gaze estimation. Cheng et al. <ref type="bibr" target="#b60">[60]</ref> propose a plug-and-play self-adversarial network to purify facial feature. Their network simultaneously removes all image feature and preserves gaze-relevant feature. As a result, this optimization mechanism implicitly removes the gaze-irrelevant feature and improve the robustness of gaze estimation networks.</p><p>Some studies crop the eye image out of the face images and directly feed it into the network. These works usually use a three-stream network to extract features from face images, left and right eye images, respectively as shown in Fig. <ref type="figure">(c</ref>) <ref type="bibr" target="#b35">[35]</ref>, <ref type="bibr" target="#b46">[46]</ref>, <ref type="bibr" target="#b61">[61]</ref>, <ref type="bibr" target="#b62">[62]</ref>, <ref type="bibr" target="#b63">[63]</ref>. Besides, Deng et al. <ref type="bibr" target="#b64">[64]</ref> decompose gaze directions into the head rotation and eyeball rotation. They use face images to estimate the head rotation and eye images to estimate the eyeball rotation. These two rotations are aggregated into a gaze vector through a gaze transformation layer. Cheng et al. <ref type="bibr" target="#b49">[49]</ref> propose a coarse-tofine gaze estimation method. They first use a CNN to extract facial features from face images and estimate a basic gaze direction, then they refine the basic gaze direction using eye features. The whole process is generalized as a bi-gram model and they use GRU <ref type="bibr" target="#b65">[65]</ref> to build the network. Facial landmarks have also been used as additional features to model the head pose and eye position. Palmero et al. directly combine individual streams (face, eyes region and face landmarks) in a CNN <ref type="bibr" target="#b66">[66]</ref>. Dias et al. extract the facial landmarks and directly regress gaze from the landmarks <ref type="bibr" target="#b67">[67]</ref>. The network outputs the gaze direction as well as an estimation of its own prediction uncertainty. Jyoti et al. further extract geometric features from the facial landmark locations <ref type="bibr" target="#b68">[68]</ref>. The geometric feature includes the angles between the pupil center as the reference point and the facial landmarks of the eyes and the tip of the nose. The detected facial landmarks can also be used for unsupervised gaze representation learning. Dubey et al. <ref type="bibr" target="#b69">[69]</ref> collect the face images from the web and annotate their gaze zone based on the detected landmarks. They perform gaze zone classification tasks on the dataset for unsupervised gaze representation learning. In addition, since the cropped face image does not contain face position information, Krafka et al. <ref type="bibr" target="#b35">[35]</ref> propose the iTracker system, which combines the information from left/right eye images, face images as well as face grid information. The face grid indicates the position of the face region in the captured image and it is usually used in PoG estimation.</p><p>3) Feature from videos: Besides the static features obtained from the images, temporal information from the videos also contributes to better gaze estimates. Recurrent Neural Network (RNN) has been widely used in video processing, e.g., long short-term memory (LSTM) <ref type="bibr" target="#b36">[36]</ref>, <ref type="bibr" target="#b70">[70]</ref>. As shown in Fig. <ref type="figure" target="#fig_6">5</ref>, they usually use a CNN to extract the features from the face images at each frame, and then input these features into a RNN. The temporal information is automatically captured by the RNN for gaze estimation.</p><p>Temporal features such as the optical flow and eye movement dynamics have been used to improve gaze estimation accuracy. The optical flow provides the motion information between the frames. Wang et al. <ref type="bibr" target="#b71">[71]</ref> use the optical flow constraints with 2D facial features to reconstruct the 3D face structure based on the input video frames. Eye movement dynamics, such as fixation, saccade and smooth pursuits, have also been used to improve gaze estimation accuracy. Wang et al. <ref type="bibr" target="#b72">[72]</ref> propose to leverage eye movement to generalize eye tracking algorithm to new subjects. They use a dynamic gaze transition network to capture underlying eye movement dynamics and serve as prior knowledge. They also propose another static gaze estimation network, which estimates gaze based on the static frame. By combining these two networks, they achieve better estimation accuracy compared with only using a static gaze estimation network. The combination method of the two networks is solved as a standard inference problem of linear dynamic system or Kalman filter <ref type="bibr" target="#b73">[73]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. CNN Models</head><p>Convolutional neural networks have been widely used in many compute vision tasks, such as object recognition <ref type="bibr" target="#b74">[74]</ref>, <ref type="bibr" target="#b75">[75]</ref> and image segmentation <ref type="bibr" target="#b76">[76]</ref>, <ref type="bibr" target="#b77">[77]</ref>, they also demonstrate superior performance in the field of gaze estimation. In this section, we first review the existing gaze estimation methods from the learning strategy perspective, i.e., the supervised CNNs and the semi-/self-/un-supervised CNNs. Then we introduce the different network architectures,i.e., multi-task CNNs and the recurrent CNNs for gaze estimation. In the last part of this section, we discuss the CNNs that integrate prior knowledge to improve performance.</p><p>1) Supervised CNNs: Supervised CNNs are the most commonly used network in appearance-based gaze estimation <ref type="bibr" target="#b26">[26]</ref>, <ref type="bibr" target="#b78">[78]</ref>, <ref type="bibr" target="#b79">[79]</ref>, <ref type="bibr" target="#b80">[80]</ref>. Fig. <ref type="figure" target="#fig_5">4</ref> also shows the typical architecture of supervised gaze estimation CNN. The network is trained using image samples with ground truth gaze directions. The gaze estimation problem is essentially learning a mapping function from raw images to the human gaze. Therefore, similar to the computer vision tasks <ref type="bibr" target="#b81">[81]</ref>, the deeper CNN architecture usually achieves better performance. A number of CNN architectures, which have been proposed for typical computer vision tasks, also show great success in gaze estimation task, e.g., LeNet <ref type="bibr" target="#b26">[26]</ref>, AlexNet <ref type="bibr" target="#b43">[43]</ref>, VGG <ref type="bibr" target="#b42">[42]</ref>, ResNet18 <ref type="bibr" target="#b36">[36]</ref> and ResNet50 <ref type="bibr" target="#b82">[82]</ref>. Besides, some well-designed modules also help to improve the estimation accuracy <ref type="bibr" target="#b46">[46]</ref>, <ref type="bibr" target="#b49">[49]</ref>, <ref type="bibr" target="#b83">[83]</ref>, <ref type="bibr" target="#b84">[84]</ref> , e.g., Chen et al. propose to use dilated convolution to extract features from eye images <ref type="bibr" target="#b46">[46]</ref>, Cheng et al. propose an attention module for fusing two eye features <ref type="bibr" target="#b49">[49]</ref>.</p><p>To supervise the CNN during training, the system requires the large-scale labeled dataset. Several large-scale datasets have been proposed, such as MPIIGaze <ref type="bibr" target="#b26">[26]</ref> and GazeCapture <ref type="bibr" target="#b35">[35]</ref>. However, it is difficult and time-consuming to collect enough gaze data in practical applications. Inspired by the physiological eye model <ref type="bibr" target="#b85">[85]</ref>, some researchers propose to synthesize labeled photo-realistic image <ref type="bibr" target="#b30">[30]</ref>, <ref type="bibr" target="#b86">[86]</ref>, <ref type="bibr" target="#b87">[87]</ref>. These methods usually build eye-region models and render new images from these models. One of such methods is proposed by Sugano et al. <ref type="bibr" target="#b30">[30]</ref>. They synthesize dense multiview eye images by recovering the 3D shape of eye regions, where they use a patch-based multi-view stereo algorithm <ref type="bibr" target="#b88">[88]</ref> to reconstruct the 3D shape from eight multi-view images. However, they did not consider the environmental changes. Wood et al. propose to synthesize the close-up eye images for a wide range of head poses, gaze directions and illuminations to develop a robust gaze estimation algorithm <ref type="bibr" target="#b89">[89]</ref>. Following this work, Wood et al. further propose another system named UnityEye to rapidly synthesize large amounts of eye images of various eye regions <ref type="bibr" target="#b90">[90]</ref>. To make the synthesized images more realistic, Shrivastava et al. propose an unsupervised learning paradigm using generative adversarial networks to improve the realism of the synthetic images <ref type="bibr" target="#b91">[91]</ref>. These methods serve as data augmentation tools to improve the performance of gaze estimation.</p><p>2) Semi-/Self-/Un-supervised CNNs: Semi-supervised, selfsupervised and unsupervised CNNs rely more on the unlabeled images to boost the gaze estimation performance. Collecting large-scale labeled images is expensive, however, it is costefficient to collect unlabeled images, they can be easily captured using web cameras.</p><p>Semi-supervised CNNs require both labeled and unlabeled images for optimizing networks. Wang et al. propose an adversarial learning approach for semi-supervised learning to improve the model performance on the target subject/dataset <ref type="bibr" target="#b51">[51]</ref>. As shown in Fig. <ref type="figure">6</ref>, it requires labeled images in the training set as well as unlabeled images of the target subject/dataset. Therefore, they annotate the source of unlabeled images as "target" and labeled images as "training set". To be more specific, they use the labeled data to supervise the gaze estimation network and design an adversarial module for semi-supervised learning. Given these features used for gaze estimation, the adversarial module tries to distinguish their source, the gaze estimation network aims to extract subject/dataset-invariant features to cheat the module.</p><p>Self-supervised CNNs aim to formulate a pretext auxiliary learning task to improve the estimation performance. Cheng et al. propose a self-supervised asymmetry regression network for gaze estimation <ref type="bibr" target="#b48">[48]</ref>. As shown in Fig. <ref type="figure">7</ref>, the network contains a regression network to estimate the two eyes' gaze directions, and an evaluation network to assess the reliability of two eyes. During training, the result of the regression network is used to supervise the evaluation network, the accuracy of the evaluation network determines the learning rate in the regression network. They simultaneously train the two networks and improve the regression performance without additional inference parameters. Xiong et al. introduce a random effect parameter to learn the person-specific information in gaze estimation <ref type="bibr" target="#b92">[92]</ref>. During training, they utilize the variational expectation-maximization algorithm <ref type="bibr" target="#b93">[93]</ref> and stochastic gradient descent <ref type="bibr" target="#b94">[94]</ref> to estimate the parameters of the random effect network. After training, they use another network to predict the random effect based on the feature representation of eye images. The self-supervised strategy predicts the random effects to enhance the accuracy for unseen subjects. He et al. introduce a person-specific user embedding mechanism. They concatenate the user embedding with appearance features to estimate gaze. They also build a teacher-student network, where the teacher network optimizes the user embedding during training and the student network learns the user embedding from the teacher network.</p><p>Unsupervised CNNs only require unlabeled data for training, nevertheless, it is hard to optimize CNNs without the ground truth. Many specific tasks are designed for unsu- Given the input image and the gaze representation difference, the gaze network reconstructs the target image. Therefore, the reconstruction task supervises the optimization of the gaze representation network. Note that, these approaches learn the gaze representation, but they also require a few labeled samples to fine-tune the final gaze estimator.</p><p>3) Multi-task CNNs: Multi-task learning usually contains multiple tasks that provide related domain information as inductive bias to improve model generalization <ref type="bibr" target="#b96">[96]</ref>, <ref type="bibr" target="#b97">[97]</ref>. Some auxiliary tasks are proposed for improving model generalization in gaze estimation. Lian et al. propose a multi-task multi-view network for gaze estimation <ref type="bibr" target="#b98">[98]</ref>. They estimate gaze directions based on single-view eye images and PoG from multi-view eye images. They also propose another multitask CNN to estimate PoG using depth images <ref type="bibr" target="#b99">[99]</ref>. They design an additional task to leverage facial features to refine depth images. The network produces four features for gaze estimation, which are extracted from the facial images, the left/right eye images and the depth images.</p><p>Some works seek to decompose the gaze into multiple related features and construct multi-task CNNs to estimate these feature. Yu et al. introduce a constrained landmarkgaze model for modeling the joint variation of eye landmark locations and gaze directions <ref type="bibr" target="#b95">[95]</ref>. As shown in Fig. <ref type="figure" target="#fig_9">9</ref>, they build a multi-task CNN to estimate the coefficients of the landmark-gaze model as well as the scale and translation information to align eye landmarks. Finally, the landmarkgaze model serve as a decode to calculate gaze from estimated parameters.. Deng et al. decompose the gaze direction into eyeball movement and head pose <ref type="bibr" target="#b64">[64]</ref>. They design a multitasks CNN to estimate the eyeball movement from eye images and the head pose from facial images. The gaze direction is computed from eyeball movement and head pose using geometric transformation. Wu et al. propose a multi-task CNN that simultaneously segments the eye part, detects the IR LED glints, and estimates the pupil and cornea center <ref type="bibr" target="#b100">[100]</ref>. The gaze direction is covered from the reconstructed eye model.</p><p>Other works perform multiple gaze-related tasks simultaneously. Recasens et al. present an approach for following gaze in video by predicting where a person (in the video) is looking, even when the object is in a different frame <ref type="bibr" target="#b101">[101]</ref>. They build a CNN to predict the gaze location in each frame and the probability containing the gazed object of each frame. Also, visual saliency shows strong correlation with human gaze in scene images <ref type="bibr" target="#b102">[102]</ref>, <ref type="bibr" target="#b103">[103]</ref>. In <ref type="bibr" target="#b104">[104]</ref>, they estimate the general visual attention and human's gaze directions in images at the same time. Kellnhofer et al. propose a dynamic 3D gaze network that includes temporal information <ref type="bibr" target="#b36">[36]</ref>. They use bi-LSTM <ref type="bibr" target="#b105">[105]</ref> to process a sequence of 7 frames. The extracted feature is used to estimate not only the gaze direction of the central frame but also the gaze uncertainty.</p><p>4) Recurrent CNNs: Human eye gaze is continuous. This inspires researchers to improve gaze estimation performance by using temporal information. Recently, recurrent neural networks have shown great capability in handling sequential data. Thus, some researchers employ recurrent CNNs to estimate the gaze in videos <ref type="bibr" target="#b66">[66]</ref>, <ref type="bibr" target="#b36">[36]</ref>, <ref type="bibr" target="#b70">[70]</ref>.</p><p>Here, we give a typical example of the data processing workflow. Given a sequence of frames {X 1 , X 2 , ..., X N }, a united CNN f U is used to extract feature vectors from each frame, i.e., x t = f U (X t ). These feature vectors are fed into a recurrent neural network f R and the network outputs the gaze vector, i.e., g i = f R (x 1 , x 2 , ..., x N ), where the index i can be set according to specific tasks, e.g., i = N <ref type="bibr" target="#b66">[66]</ref> or i = (N + 1)/2 <ref type="bibr" target="#b36">[36]</ref>. An example is also shown in Fig. <ref type="figure" target="#fig_6">5</ref>.</p><p>Different types of input have been explored to extract features. Kellnhofer et al. directly extract features from facial images <ref type="bibr" target="#b36">[36]</ref>. Zhou et al. combine the feature extracted from facial and eye images <ref type="bibr" target="#b70">[70]</ref>. Palmero et al. use facial images, binocular images and facial landmarks to generate the feature vectors <ref type="bibr" target="#b66">[66]</ref>. Different RNN structures have also been explored, such as GRU <ref type="bibr" target="#b65">[65]</ref> in <ref type="bibr" target="#b66">[66]</ref>, LSTM <ref type="bibr" target="#b106">[106]</ref> in <ref type="bibr" target="#b70">[70]</ref> and bi-LSTM <ref type="bibr" target="#b105">[105]</ref> in <ref type="bibr" target="#b36">[36]</ref>. Cheng et al. leverage the recurrent CNN to improve gaze estimation performance from static images rather than videos <ref type="bibr" target="#b49">[49]</ref>. They generalize the gaze estimation as a sequential coarse-to-fine process and use GRU to relate the basic gaze direction estimated from facial images and the gaze residual estimated from eye images.</p><p>5) CNNs With Other Priors: Prior information also helps to improve gaze estimation accuracy, such as eye models, eye movement patterns, etc. <ref type="bibr" target="#b64">[64]</ref>, <ref type="bibr" target="#b48">[48]</ref>, <ref type="bibr" target="#b38">[38]</ref>, <ref type="bibr" target="#b72">[72]</ref>, <ref type="bibr" target="#b92">[92]</ref>, <ref type="bibr" target="#b107">[107]</ref>.</p><p>Decomposition of Gaze Direction. The human gaze can be decomposed into the head pose and the eyeball orientation. Deng et al. use two CNNs to respectively estimate the head pose from facial images and the eyeball orientation from eye images. Then, they integrate the two results into final gaze with geometric transformation <ref type="bibr" target="#b64">[64]</ref>.</p><p>Anatomical Eye Model. The human eye is composed of the eye ball, the iris, and the pupil center, etc. Park et al. propose a pictorial gaze representation based on the eye model to predict the gaze direction <ref type="bibr" target="#b38">[38]</ref>. They render the eye model to generate a pictorial image, where the pictorial image eliminates the appearance variance. They use a CNN to map the original images into the pictorial images and use another CNN to estimate gaze directions from the pictorial image. Eye Movement Pattern. Common eye movements, such as fixation, saccade and smooth pursuits, are independent of viewing contents and subjects. Wang et al. propose to incorporate the generic eye movement pattern in dynamic gaze estimation <ref type="bibr" target="#b72">[72]</ref>. They recover the eye movement pattern from videos and use a CNN to estimate gaze from static images.</p><p>Two eye asymmetry Property. Cheng et al. discover the 'two eye asymmetry' property that the appearances of two eyes are different while the gaze directions of two eyes are approximately the same <ref type="bibr" target="#b37">[37]</ref>. Based on this observation, Cheng et al. propose to treat the two eyes asymmetrically in the CNN. They design an asymmetry regression network for adaptive weighting two eyes based on their performance. They also design an evaluation network for evaluating the asymmetric state of the regression network.</p><p>Gaze data distribution. The basic assumption of most regression model is independent identically distributed (i.i.d), however, gaze data is not i.i.d. Xiong et al. discuss the noni.i.d problem in <ref type="bibr" target="#b92">[92]</ref>. They design a mixed-effect model to take into account the person-specific information.</p><p>Inter-subject bias. Chen et al. observe the inter-subject bias in most datasets <ref type="bibr" target="#b107">[107]</ref>. They make the assumption that there exists a subject-dependent bias that cannot be estimated from images. Thus, they propose a gaze decomposition method. They decompose the gaze into the subject-dependent bias and the subject-independent gaze estimated from images. During test, they use some image samples to calibrate the subjectdependent bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Personal Calibration</head><p>It is non-trivial to learn an accurate and universal gaze estimation model. Conventional 3D eye model recovery methods usually build a unified gaze model including subjectspecific parameters such as eyeball radius <ref type="bibr" target="#b22">[22]</ref>. They perform a personal calibration to estimate these subject-specific parameters. In the field of deep learning-based gaze estimation, personal calibration is also explored to improve person-specific performance. Fig. <ref type="figure" target="#fig_10">10</ref> shows a common pipeline of personal calibration in deep learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Calibration via Domain Adaptation:</head><p>The calibration problem can be considered as domain adaption problems, where the training set is the source domain and the test set is the target domain. The test set usually contains unseen subjects (the cross-person problem), or unseen environment (the crossdataset problem). Researchers aim to improve the performance in the target domain using the calibration samples.</p><p>The common approach of domain adaption is to fine-tune the model in the target domain <ref type="bibr" target="#b35">[35]</ref>, <ref type="bibr" target="#b108">[108]</ref>, <ref type="bibr" target="#b109">[109]</ref>. This is simple but effective. Krafka et al. replace the fully-connected layer with an SVM and fine-tune the SVM layer to predict the gaze location <ref type="bibr" target="#b35">[35]</ref>. Zhang et al. split the CNN into three parts: the encoder, the feature extractor, and the decoder <ref type="bibr" target="#b108">[108]</ref>. They fine-tune the encoder and decoder in each target domain. Zhang et al. also learn a third-order polynomial mapping function between the estimated and ground-truth of 2D gaze locations <ref type="bibr" target="#b5">[5]</ref>. Some studies introduce person-specific feature for gaze estimation <ref type="bibr" target="#b110">[110]</ref>, <ref type="bibr" target="#b111">[111]</ref>. They learn the personspecific feature during fine-tuning. Linden et al. introduce user embedding for recording personal information. They obtain user embedding of the unseen subjects by fine-tuning using calibration samples <ref type="bibr" target="#b110">[110]</ref>. Chen et al. <ref type="bibr" target="#b107">[107]</ref> observe the different gaze distributions of subjects. They use the calibration samples to estimate the bias between the estimated gaze and the ground-truth of different subjects. They use bias to refine the estimates. In addition, Yu et al. generate additional calibration samples through the synthesis of gaze-redirected eye images from the existing calibration samples <ref type="bibr" target="#b39">[39]</ref>. The generated samples are also directly used for training. These methods all need labeled samples for supervised calibration.</p><p>Besides the supervised calibration methods, there are some unsupervised calibration methods. These methods use unlabeled calibration samples to improve performance. They usually seek to align the features in different domains. <ref type="bibr">Wang et al.</ref> propose an adversarial method for aligning features. They build a discriminator to judge the source of images from the extracted feature. The feature extractor has to confuse the discriminator, i.e., the generated feature should be domaininvariant. The adversarial method is semi-supervised and does not require labeled calibration samples. Guo et al. <ref type="bibr" target="#b112">[112]</ref> use source samples to form a locally linear representation of each target domain prediction in gaze space. The same linear relationships are applied in the feature space to generate the feature representation of target samples. Meanwhile, they minimize the difference between the generated feature and extracted feature of target sample for alignment. Cheng et al. <ref type="bibr" target="#b60">[60]</ref> propose a domain generalization methods. They improve the corss-dataset performance without knowing the target dataset or touching any new samples. They propose a self-adversarial framework to remove the gaze-irrelevant feature in face images. Since the gaze pattern is invariant in different domains, they align the features in different domains. Cui et al. define a new adaption problem <ref type="bibr" target="#b113">[113]</ref>: adaptation from adults to children. They use the conventional domain adaption method, geodesic flow kernel <ref type="bibr" target="#b114">[114]</ref> a meta learning-based calibration approach <ref type="bibr" target="#b40">[40]</ref>. They train a highly adaptable gaze estimation network through meta learning. The network can be converted into a person-specific network once training with target person samples. Liu et al. propose a differential CNN based on metric learning <ref type="bibr" target="#b115">[115]</ref>. The network predicts the gaze difference between two eyes. For inference, they have a set of subject-specific calibration images. Given a new image, the network estimates the differences between the given image and the calibration image, and takes the average of them as the final estimated gaze.</p><p>2) Calibration via User-unaware Data Collection: Most calibration-based methods require labeled samples. However, it is difficult to acquire enough labeled samples in practical applications. Collecting calibration samples in a user-unaware manner is an alternative solution <ref type="bibr" target="#b116">[116]</ref>, <ref type="bibr" target="#b117">[117]</ref>, <ref type="bibr" target="#b118">[118]</ref>.</p><p>Some researchers implicitly collect calibration data when users are using computers. Salvalaio et al. propose to collect data when the user is clicking a mouse, this is based on the assumption that users are gazing at the position of the cursor when clicking the mouse <ref type="bibr" target="#b118">[118]</ref>. They use online learning to fine-tune their model with the calibration samples.</p><p>Other studies investigate the relation between the gaze points and the saliency maps <ref type="bibr" target="#b102">[102]</ref>, <ref type="bibr" target="#b103">[103]</ref>. Chang et al. utilize saliency information to adapt the gaze cestimation algorithm to a new user without explicit calibration <ref type="bibr" target="#b116">[116]</ref>. They transform the saliency map into a differentiable loss map that can be used to optimize the CNN models. Wang et al. introduce a stochastic calibration procedure. They minimize the difference between the probability distribution of the predicted gaze and the ground truth <ref type="bibr" target="#b117">[117]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Devices and Platforms</head><p>1) Camera: The majority of gaze estimation systems use a single RGB camera to capture eye images, while some studies use different camera settings, e.g., using multiple cameras to capture multi-view images <ref type="bibr" target="#b98">[98]</ref>, <ref type="bibr" target="#b119">[119]</ref>, using infrared (IR) cameras to handle low illumination condition <ref type="bibr" target="#b100">[100]</ref>, <ref type="bibr" target="#b121">[121]</ref>, and using RGBD cameras to provide the depth information <ref type="bibr" target="#b99">[99]</ref>. Different cameras and their captured images are shown in Fig. <ref type="figure" target="#fig_11">11</ref>.</p><p>Tonsen et al. embed multiple millimeter-sized RGB cameras into a normal glasses frame <ref type="bibr" target="#b119">[119]</ref>. They use multi-layer  perceptrons to process the eye images captured by different cameras, and concatenate the extracted feature to estimate gaze. Lian et al. mount three cameras at the bottom of a screen <ref type="bibr" target="#b98">[98]</ref>. They build a multi-branch network to extract the features of each view and concatenate them to estimate 2D gaze position on the screen. Wu et al. collect gaze data using near-eye IR cameras <ref type="bibr" target="#b100">[100]</ref>. They use CNN to detect the location of glints, pupil centers and corneas from IR images. Then, they build an eye model using the detected feature and estimate gaze from the gaze model. Kim et al. collect a largescale dataset of near-eye IR eye images <ref type="bibr" target="#b121">[121]</ref>. They synthesize additional IR eye images that cover large variations in face shape, gaze direction, pupil and iris etc.. Lian et al. use RGBD cameras to capture depth facial images <ref type="bibr" target="#b99">[99]</ref>. They extract the depth information of eye regions and concatenate it with RGB image features to estimate gaze.</p><p>2) Platform: Eye gaze can be used to estimate human intent in various applications, e.g., product design evaluation <ref type="bibr" target="#b126">[126]</ref>, marketing studies <ref type="bibr" target="#b127">[127]</ref> and human-computer interaction <ref type="bibr" target="#b128">[128]</ref>, <ref type="bibr" target="#b129">[129]</ref>, <ref type="bibr" target="#b7">[7]</ref>. These applications can be categorized into three types of platforms: computers, mobile devices and head-mounted devices. We summarize the characteristics of these platforms in Fig. <ref type="figure" target="#fig_1">12</ref>.</p><p>The computer is the most common platform for appearancebased gaze estimation. The cameras are usually placed below/above the computer screen <ref type="bibr" target="#b26">[26]</ref>, <ref type="bibr" target="#b130">[130]</ref>, <ref type="bibr" target="#b48">[48]</ref>, <ref type="bibr" target="#b49">[49]</ref>, <ref type="bibr" target="#b38">[38]</ref>. Some works focus on using deeper neural networks <ref type="bibr" target="#b26">[26]</ref>, <ref type="bibr" target="#b43">[43]</ref>, <ref type="bibr" target="#b47">[47]</ref> or extra modules <ref type="bibr" target="#b38">[38]</ref>, <ref type="bibr" target="#b48">[48]</ref>, <ref type="bibr" target="#b49">[49]</ref> to improve gaze estimation performance, while the other studies seek to use custom devices for gaze estimation, such as multi-cameras and RGBD cameras <ref type="bibr" target="#b98">[98]</ref>, <ref type="bibr" target="#b99">[99]</ref>.</p><p>The mobile device is another common platform for gaze estimation <ref type="bibr" target="#b35">[35]</ref>, <ref type="bibr" target="#b50">[50]</ref>, <ref type="bibr" target="#b124">[124]</ref>. Such devices often contain front cameras, but the computational resources are limited. These systems usually estimate PoG instead of gaze directions due to the difficulty of geometric calibration. Krafka et al. propose a PoG estimation method for mobile devices, named iTracker <ref type="bibr" target="#b35">[35]</ref>. They combine the facial image, two eye images </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Perspectives Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2015">2016 2019</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature</head><p>Eye image <ref type="bibr" target="#b26">[26]</ref> - <ref type="bibr" target="#b42">[42]</ref>, <ref type="bibr" target="#b119">[119]</ref>, <ref type="bibr" target="#b113">[113]</ref> Semi-/Self-/Un-Supervised CNN --- <ref type="bibr" target="#b48">[48]</ref> [51], <ref type="bibr" target="#b69">[69]</ref>, <ref type="bibr" target="#b92">[92]</ref>, <ref type="bibr" target="#b110">[110]</ref>, <ref type="bibr" target="#b111">[111]</ref> [54], <ref type="bibr" target="#b112">[112]</ref> - <ref type="bibr" target="#b43">[43]</ref>, <ref type="bibr" target="#b64">[64]</ref>, <ref type="bibr" target="#b42">[42]</ref>, <ref type="bibr" target="#b113">[113]</ref> [38], <ref type="bibr" target="#b47">[47]</ref>, <ref type="bibr" target="#b48">[48]</ref>, <ref type="bibr" target="#b66">[66]</ref>, <ref type="bibr" target="#b68">[68]</ref>, <ref type="bibr" target="#b95">[95]</ref>, <ref type="bibr" target="#b104">[104]</ref>, <ref type="bibr" target="#b108">[108]</ref> [5], <ref type="bibr" target="#b36">[36]</ref>, <ref type="bibr" target="#b40">[40]</ref>, <ref type="bibr" target="#b46">[46]</ref>, <ref type="bibr" target="#b69">[69]</ref>, <ref type="bibr" target="#b70">[70]</ref>, <ref type="bibr" target="#b71">[71]</ref>, <ref type="bibr" target="#b72">[72]</ref>, <ref type="bibr" target="#b98">[98]</ref>, <ref type="bibr" target="#b99">[99]</ref>, <ref type="bibr" target="#b115">[115]</ref>, <ref type="bibr" target="#b116">[116]</ref>, <ref type="bibr" target="#b118">[118]</ref> [49], <ref type="bibr" target="#b59">[59]</ref>, <ref type="bibr" target="#b61">[61]</ref>, <ref type="bibr" target="#b83">[83]</ref>, <ref type="bibr" target="#b62">[62]</ref>, <ref type="bibr" target="#b123">[123]</ref>, <ref type="bibr" target="#b78">[78]</ref>, <ref type="bibr" target="#b79">[79]</ref>, <ref type="bibr" target="#b52">[52]</ref>, <ref type="bibr" target="#b53">[53]</ref>, <ref type="bibr" target="#b80">[80]</ref>, <ref type="bibr" target="#b63">[63]</ref>, <ref type="bibr">[125]</ref>, <ref type="bibr" target="#b82">[82]</ref>, <ref type="bibr" target="#b37">[37]</ref>, and the face grid to estimate the gaze. The face grid encodes the position of the face in captured images and is proved to be effective for gaze estimation in mobile devices in many works <ref type="bibr" target="#b111">[111]</ref>, <ref type="bibr" target="#b50">[50]</ref>. He et al. propose a more accurate and faster method based on iTracker <ref type="bibr" target="#b111">[111]</ref>. They replace the face grid with a more sensitive eye corner landmark feature. Guo et al. propose a generalized gaze estimation method <ref type="bibr" target="#b122">[122]</ref>. They propose a tolerant training scheme, the knowledge distillation framework. They observe the notable jittering problem in gaze point estimates and propose to use adversarial training to address this problem.</p><p>The head-mounted device usually employs near-eye cameras to capture eye images. Tonsen et al. embed millimetre-sized RGB cameras into a normal glasses frame <ref type="bibr" target="#b119">[119]</ref>. In order to compensate for the low-resolution captured images, they use multi-cameras to capture multi-view images and use a neural network to regress gaze from these images. IR cameras are also employed by head-mounted devices. Wu et al. collect the MagicEyes dataset using IR cameras <ref type="bibr" target="#b100">[100]</ref>. They propose EyeNet, a neural network that solves multiple heterogeneous </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HCS HCS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Origin images Normalized images</head><p>Illustrate the normalization result on images Reference point Fig. <ref type="figure" target="#fig_3">13</ref>. A data rectification method <ref type="bibr" target="#b30">[30]</ref>. The virtual camera is rotated so that the z-axis points at the reference point and the x-axis is parallel with the x -axis of the head coordinate system (HCS). The bottom row illustrates the rectification result on images. Overall, the reference point is moved to the center of images, the image is rotated to straighten face and scaled to align the size of face in different images.</p><p>tasks related to eye gaze estimation for an off-axis camera setting. They use the CNN to model 3D cornea and 3D pupil and estimate the gaze from these two 3D models. Lemley et al. use the single near-eye image as input to the neural network and directly regress gaze <ref type="bibr" target="#b120">[120]</ref>. Kim et al. follow a similar approach and collect the NVGaze dataset <ref type="bibr" target="#b121">[121]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Summarization</head><p>We also summarizes and categorize the existing CNNbased gaze estimation methods in Tab. I. Note that, many methods do not specify a platform <ref type="bibr" target="#b26">[26]</ref>, <ref type="bibr" target="#b49">[49]</ref>. We categorize these methods all into "computer" in the row of platform. In summary, there is an increasing trend in developing supervised or semi-/self-/un-supervised CNN structures to estimate gaze. More recent research interests shift to different calibration approaches through domain adaptation or user-unaware data collection. The first CNN-based gaze direction estimation method is proposed by Zhang et al. in 2015 <ref type="bibr" target="#b26">[26]</ref>, the first CNN-based PoG estimation method is proposed by Krafka et al. in 2016 <ref type="bibr" target="#b35">[35]</ref>. These two studies both provide large-scale gaze datasets, the MPIIGaze and the GazeCapture, which have been widely used for evaluating gaze estimation algorithms in later studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. DATASETS AND BENCHMARKS</head><p>A. Data Pre-processing 1) Face and Eye Detection: Raw images often contain unnecessary information for gaze estimation, such as the background. Directly using raw images to regress gaze not only increases the computational resource but also brings nuisance factors such as changes in scenes. Therefore, face or eye detection is usually applied in raw images to prune unnecessary information. Generally, researchers first perform face alignment in raw images to obtain facial landmarks and crop face/eye images using these landmarks. Several face alignment methods have been proposed recently <ref type="bibr" target="#b137">[137]</ref>, <ref type="bibr" target="#b138">[138]</ref>, <ref type="bibr" target="#b139">[139]</ref>. We list some typical face alignment methods in Tab. II.</p><p>After the facial landmarks are obtained, face or eye image are cropped accordingly. There is no protocol to regulate the cropping procedure. We provide a common cropping procedure here as an example. We let x i ∈ R 2 be the x, ycoordinates of the ith facial landmark in an raw image I. The center point x is calculated as x = 1 n n i=1 x i , where n is the number of facial landmarks. The face image is defined as a square region with the center x and an width w. The w is usually set empirically. For example, <ref type="bibr" target="#b43">[43]</ref> set w as 1.5 times of the maximum distance between the landmarks. The eye cropping is similar to face cropping, while the eye region is usually defined as a rectangle with the center set as the centroid of eye landmarks. The width of the rectangle is set based the distance between eye corners, e.g., 1.2 times.</p><p>2) Data Rectification: Gaze data is usually collected in the laboratory environment. Subjects are required to fix their head on a chin rest <ref type="bibr" target="#b140">[140]</ref>. Recent research gradually shifts the attention from the constrained gaze estimation to unconstrained gaze estimation. The unconstrained gaze estimation introduces many environmental factors such as illumination and background. These factors increase the complexity of eye appearance and complicate the gaze estimation problem. Although the CNNs have a strong fitting ability, it is still difficult to achieve accurate gaze estimation in an unconstrained environment. The goal of data rectification is to eliminate the environmental factors by data pre-processing methods and to simplify the gaze regression problem. Current data rectification methods mainly focus on head pose and illumination factors.</p><p>The head pose can be decomposed into the rotation and translation of the head. The change of head pose degrades eye appearance and introduces ambiguity on eye appearance. To handle the head pose changes, Sugano et al. propose to rectify the eye image by rotating the virtual camera to point at the same reference point in the human face <ref type="bibr" target="#b30">[30]</ref>. They assume that the captured eye image is a plane in 3D space, the rotation of the virtual camera can be performed as a perspective transformation on the image. The whole data </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rs ∈ R 3×3</head><p>The rotation matrix of SCS w.r.t. CCS.</p><p>Ts ∈ R 3 Ts = (tx, ty, tz), the translation matrix of SCS w.r.t. CCS.</p><p>n ∈ R 3 n = (nx, ny, nz), the normal vectors of x-y plane of SCS.</p><p>rectification process is shown in Figure <ref type="figure" target="#fig_3">13</ref>. They compute the transformation matrix M = SR, where R is the rotation matrix and S is the scale matrix. R also indicates the rotated camera coordinate system. The z-axis z c of the rotated camera coordinate system is defined as the line from cameras to reference points, where the reference point is usually set as the face center or eye center. It means that the rotated camera is pointing towards the reference point. The rotated x-axis x c is defined as the x-axis of the head coordinate system so that the appearance captured by the rotated cameras is facing the front. The rotated y-axis y c can be computed by y c = z c × x c , the x c is recalculated by x c = y c × z c to maintain orthogonality. As a result, the rotation matrix R = [ xc ||xc|| , yc ||yc|| , zc ||zc|| ]. The S maintains the distance between the virtual camera and the reference point, which is defined as diag(1, 1, dn do ), where d o is the original distance between the camera and the reference point, and d n is the new distance that can be adjusted manually. They apply a perspective transformation on images with W = C n M C −1 r , where C r is the intrinsic matrix of the original camera and C n is the intrinsic matrix of the new camera. Gaze directions can also be calculated in the rotated camera coordinate system asĝ = M g. The method eliminates the ambiguity caused by different head positions and aligns the intrinsic matrix of cameras. It also rotates the captured image to cancel the degree of freedom of roll in head rotation. Zhang et al. further explore the method in <ref type="bibr" target="#b142">[141]</ref>. They argue that scaling can not change the gaze direction vector. The gaze direction is computed byĝ = Rg.</p><p>Illumination also influences the appearance of the human eye. To handle this, researchers usually take gray-scale images rather than RGB images as input and apply histogram equalization in the gray-scale images to enhance the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Data Post-processing</head><p>Various applications demand different forms of gaze estimates. For example, in a real-world interaction task, it requires 3D gaze directions to indicate the human intent <ref type="bibr" target="#b143">[142]</ref>, <ref type="bibr" target="#b10">[10]</ref>, while it requires 2D PoG for the screen-based interaction <ref type="bibr" target="#b7">[7]</ref>, <ref type="bibr" target="#b144">[143]</ref>. In this section, we introduce how to convert different forms of gaze estimates by post-processing. We list the symbols in Tab. III and illustrate the symbols in Fig. <ref type="figure" target="#fig_5">14</ref>. We denote the PoG as 2D gaze and the gaze direction as 3D gaze in this section.</p><p>1) 2D/3D Gaze Conversion: The 2D gaze estimation algorithm usually estimates gaze targets on a computer screen <ref type="bibr" target="#b122">[122]</ref>, <ref type="bibr" target="#b72">[72]</ref>, <ref type="bibr" target="#b35">[35]</ref>, <ref type="bibr" target="#b116">[116]</ref>, <ref type="bibr" target="#b145">[144]</ref>, while the 3D gaze estimation algorithm estimates gaze directions in 3D space <ref type="bibr" target="#b42">[42]</ref>, <ref type="bibr" target="#b43">[43]</ref>, <ref type="bibr" target="#b36">[36]</ref>, <ref type="bibr" target="#b49">[49]</ref>, <ref type="bibr" target="#b92">[92]</ref>. We first introduce how to convert between the 2D gaze and the 3D gaze.</p><p>Given a 2D gaze target p = (u, v) on the screen, our goal is to compute the corresponding 3D gaze direction g = (g x , g y , g z ). The processing pipeline is that we first compute the 3D gaze target t and 3D gaze origin o in the camera coordinate system (CCS). The gaze direction can be computed as</p><formula xml:id="formula_1">g = t − o ||t − o|| . (<label>1</label></formula><formula xml:id="formula_2">)</formula><p>To derive the 3D gaze target t, we obtain the pose {R s , T s } of screen coordinate system (SCS) w.r.t. CCS by geometric calibration, where R s is the rotation matrix and T s is the translation matrix. The t is computed as t = R s [u, v, 0] T +T s , where the additional 0 is the z-axis coordinate of p in SCS. The 3D gaze origin o is usually defined as the face center or the eye center. It can be estimated by landmark detection algorithms or stereo measurement methods.</p><p>On the other hand, given a 3D gaze direction g, we aim to compute the corresponding 2D target point p on the screen. Note that, we also need to acquire the screen pose {R s , T s } as well as the origin point o as mentioned previously. We first compute the intersection of gaze direction and screen, i.e., 3D gaze target t, in CCS, and then we convert the 3D gaze target to the 2D gaze target using the pose {R s , T s }.</p><p>To deduce the equation of screen plane, we compute n = R s [:, 2] = (n x , n y , n z ), where n is the normal vector of screen plane. T s = [t x , t y , t z ] T also represents a point on the screen plane. Therefore, the equation of the screen plane is</p><formula xml:id="formula_3">n x x + n y y + n z z = n x t x + n y t y + n z t z .</formula><p>(2)</p><p>Given a gaze direction g and the origin point o, we can write the equation of the line of sight as  By solving Eq. (2) and Eq. (3), we obtain the intersection t, and (u, v, z) = R s −1 (t − T s ), where z usually equals to 0 and p = (u, v) is the coordinate of 2D target point in metre.</p><formula xml:id="formula_4">x − x o g x = y − y o g y = z − z o g z . (<label>3</label></formula><formula xml:id="formula_5">)</formula><p>2) Gaze Origin Conversion: Conventional gaze estimation methods usually estimate gaze directions w.r.t. each eye. They define the origin of gaze directions as each eye center <ref type="bibr" target="#b42">[42]</ref>, <ref type="bibr" target="#b38">[38]</ref>, <ref type="bibr" target="#b51">[51]</ref>, <ref type="bibr" target="#b115">[115]</ref>. Recently, more attention has been paid to gaze estimation using face images, they usually estimate one gaze direction w.r.t.the whole face. They define the gaze direction as the vector starting from the face center to the gaze target <ref type="bibr" target="#b49">[49]</ref>, <ref type="bibr" target="#b40">[40]</ref>, <ref type="bibr" target="#b43">[43]</ref>, <ref type="bibr" target="#b47">[47]</ref>. Here, we introduce a gaze origin conversion method to bridge the gap between these two types of gaze estimates.</p><p>We first compute the pose {R s , T s } of SCS and the origin o of the predicted gaze direction g through calibration. Then we can write Eq. (2) and Eq. (3) based on these parameters. The 3D gaze target point t can be calculated by solving the equation of Eq. (2) and Eq. (3). Next, we obtain the new origin o n of the gaze direction through 3D landmark detection. The new gaze direction can be computed by</p><formula xml:id="formula_6">g new = t − o n ||t − o n || .<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Evaluation Metrics</head><p>Two types of metric are used for performance evaluation: the angular error and the Euclidean distance. Two kinds of evaluation protocols are commonly used: within-dataset evaluation and cross-dataset evaluation.</p><p>The angular error is usually used for measuring the accuracy of 3D gaze estimation method <ref type="bibr" target="#b49">[49]</ref>, <ref type="bibr" target="#b40">[40]</ref>, <ref type="bibr" target="#b42">[42]</ref>. Assuming the actual gaze direction is g ∈ R 3 and the estimated gaze direction isĝ ∈ R 3 , the angular error can be computed as:</p><formula xml:id="formula_7">L angular = g •ĝ ||g||||ĝ|| .<label>(5)</label></formula><p>The Euclidean distance has been used for measuring the accuracy of 2D gaze estimation methods in <ref type="bibr" target="#b35">[35]</ref>, <ref type="bibr" target="#b116">[116]</ref>, <ref type="bibr" target="#b145">[144]</ref>. We denote the actual gaze position as p ∈ R 2 and the estimated gaze position asp ∈ R 2 . We can compute the Euclidean distance as</p><formula xml:id="formula_8">L Euclidean = ||p −p||,<label>(6)</label></formula><p>The within-dataset evaluation assesses the model performance on the unseen subjects from the same dataset. The dataset is divided into the training set and the test set according to the subjects. There is no intersection of subjects between the training set and test set. Note that, most of the gaze datasets provide within-dataset evaluation protocol. They divide the data into training set and test set in advance.</p><p>The cross-dataset evaluation assesses the model performance on the unseen environment. The model is trained on one dataset and tested on another dataset.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Public Datasets</head><p>Many large-scale gaze datasets have been proposed. In this survey, we try our best to summarize all the public datasets on gaze estimation, as shown in Tab. IV. The gaze direction distribution and head pose distribution of these datasets are shown in Fig. <ref type="figure" target="#fig_6">15</ref>. Note that, the Gaze360 dataset do not provide the head information. We also discuss three typical datasets that are widely used in gaze estimation studies.</p><p>1) MPIIGaze: Zhang et al. proposed the MPIIGaze <ref type="bibr" target="#b26">[26]</ref> dataset. It is the most popular dataset for appearance-based gaze estimation methods. The MPIIGaze dataset contains a total of 213,659 images collected from 15 subjects. They are collected in daily life over several months and there is no constraint for the head pose. As a result, the images are of different illumination and head poses. The MPIIGaze dataset provides both 2D and 3D gaze annotation. It also provides a standard evaluation set. The evaluation set contains 15 subjects and 3,000 images for each subject. The images are consisted of 1,500 left-eye images and 1,500 right-eye images from 15 subjects. The author further extends the original datasets in <ref type="bibr" target="#b43">[43]</ref>, <ref type="bibr" target="#b42">[42]</ref>. The original MPIIGaze dataset only provides binocular eye images, while they supply the corresponding face images in <ref type="bibr" target="#b43">[43]</ref> and manual landmark annotations in <ref type="bibr" target="#b42">[42]</ref>.</p><p>2) EyeDiap: EyeDiap <ref type="bibr" target="#b146">[145]</ref> dataset consists of 94 video clips from 16 participants. Different from MPIIGaze, the EyeDiap dataset is collected in a laboratory environment. It has three visual target sessions: the continuous moving target, the discrete moving target, and the floating ball. For each subject, they recorded a total of six sessions containing two head movements: static head pose and free head movement. Two cameras are used for data collection: an RGBD camera and an HD camera. The disadvantage of this dataset is that it lacks variation in illumination.</p><p>3) GazeCapture: The GazeCapture <ref type="bibr" target="#b35">[35]</ref> dataset is collected through crowdsourcing. It contains a total of 2,445,504 images from 1,474 participants. All images are collected using mobile phones or tablets. Each participant is required to gaze at a circle shown on the devices without any constraint on their head movement. As a result, the GazeCapture dataset covers various lighting conditions and head motions. The GazeCapture dataset does not provide 3D coordinates of the targets. It is usually used for the evaluation of unconstrained 2D gaze point estimation methods.</p><p>In addition to the dataset mentioned above, there are several datasets being proposed recently. In 2018, Fischer et al. proposed RT-Gene dataset <ref type="bibr" target="#b47">[47]</ref>. This dataset provides accurate 3D gaze data since they collect gaze with a dedicated eye tracking device. In 2019, Kellnhofe et al. proposed the Gaze360 dataset <ref type="bibr" target="#b36">[36]</ref>. The dataset consists of 238 subjects of indoor and outdoor environments with 3D gaze across a wide range of head poses and distances. In 2020, Zhang et al. propose the ETH-XGaze dataset <ref type="bibr" target="#b82">[82]</ref>. This dataset provides high-resolution images that cover extreme head poses. It also contains 16 illumination conditions for exploring the effects of illumination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Benchmarks</head><p>The system setups of the gaze estimation models are different. 2D PoG estimation and 3D gaze direction estimation are two popular gaze estimation tasks. In addition, with regard to 3D gaze direction estimation, some methods are designed for estimating gaze from eye images. They define the origin of gaze directions as eye centers. However, this definition is not suitable for methods estimating gaze from face images. Therefore, these methods slightly change the definition of gaze directions and define the origin of gaze direction as face centers. These different task definitions become a barrier to compare gaze estimation methods. In this section, we break through the barrier with the data post-processing method, and build a comprehensive benchmark.</p><p>We conduct benchmarks in three common gaze estimation tasks: a) estimate gaze directions originating from eye centers. b) estimate gaze directions originating from face centers. c) estimate 2D PoG. The results are shown in Figure <ref type="figure">V</ref> and Figure VI. We implement the typical gaze estimation methods of the three tasks. We use the gaze origin conversion method to convert the results of task a and task b, and use the 2D/3D gaze conversion method to convert the results of task c and task a/b. The two conversion methods are introduced in Section IV-B. The data pre-processing of each datasets and implemented method are available at http://phi-ai.org/GazeHub.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS AND FUTURE DIRECTIONS</head><p>In this survey, we present a comprehensive overview of deep learning-based gaze estimation methods. Unlike the conventional gaze estimation methods that requires dedicated devices, the deep learning-based approaches regress the gaze from the eye appearance captured by web cameras. This makes it easy to implement the algorithm in real world applications. We introduce the gaze estimation method from four perspectives: deep feature extraction, deep neural network architecture design, personal calibration as well as device and platform. We summarize the public datasets on appearancebased gaze estimation and provide benchmarks to compare of the state-of-the-art algorithms. This survey can serve as a guideline for future gaze estimation research.</p><p>Here, we further suggest several future directions of deep learning-based gaze estimation. 1) Extracting more robust gaze features. The perfect gaze estimation method should be accurate under all different subjects, head poses, and environments. Therefore, a environment-invariant gaze feature is critical. 2) Improve performance with fast and simple calibration. There is a trade-off between the system performance and calibration time. The longer calibration time leads to more accurate estimates. How to achieve satisfactory performance with fast calibration procedure is a promising direction. 3) Interpretation of the learned features. Deep learning approach often serves as a black box in gaze estimation problem. Interpretation of the learned features in these methods brings insight for the deep learning-based gaze estimation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. (a)The change of devices in gaze estimation. From intrusive skin electrodes<ref type="bibr" target="#b11">[11]</ref> to off-shelf web cameras, gaze estimation becomes more convenient. (b) Gaze estimation methods are also updated with the change of devices. We illustrate five kinds of gaze estimation methods.<ref type="bibr" target="#b1">(1)</ref>. Attached sensor based methods. The method samples the electrical signal of skin electrodes. The signal indicates the eye movement of subjects<ref type="bibr" target="#b12">[12]</ref>. (2) 3D eye model recovery methods. The method usually builds a geometric eye model to calculate the visual axis, i.e., gaze directions. The eye model is fitted based on the light reflection. (3) 2D eye feature regression methods. The method relies on IR cameras to detect geometric eye features such as pupil center, glints, and directly regress the PoG from these features. (4) Conventional appearance-based methods. The method use entire images as feature and directly regress human gaze from features. Some feature reduction methods are also used for extracting low-dimensional feature. For example, Lu et al. divide eye images into 15 subregion and sum the pixel intensities in each subregion as feature<ref type="bibr" target="#b13">[13]</ref>. (5) Appearance-based gaze estimation with deep learning. Face or eye images are directly inputted into a designed neural network to estimate human gaze.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig.3. The architecture of section 3. We introduce gaze estimation with deep learning from four perspectives.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Gaze estimation with face and eye images<ref type="bibr" target="#b35">[35]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>Fig.4. Some typical CNN based gaze estimation network. (a). Gaze estimation with eye images<ref type="bibr" target="#b42">[42]</ref>. (b) Gaze estimation with face images<ref type="bibr" target="#b43">[43]</ref>. (c). Gaze estimation with face and eye images<ref type="bibr" target="#b35">[35]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>Fig.<ref type="bibr" target="#b5">5</ref>. Gaze estimation with videos. It first extracts static features from each frame using a typical CNN, and feeds these static features into RNN for extracting temporal information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .Fig. 7 .</head><label>67</label><figDesc>Fig. 6. A semi-supervised CNN [51]. It uses both labeled images and unlabeled images for training. It designs an extra appearance classifier and a head pose classifier. The two classifiers align the feature of labeled images and unlabeled images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>Fig.8. A unsupervised CNN<ref type="bibr" target="#b54">[54]</ref>. It uses a CNN to extract 2-D feature from eye images. The feature difference of two images and one of eye image are fed into a pretrained gaze redirection network to generate the other eye image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 .</head><label>9</label><figDesc>Fig.9. A multitask CNN<ref type="bibr" target="#b95">[95]</ref>. It estimates the coefficients of a landmark-gaze model as well as the scale and translation parameters to align eye landmarks. The three results are used to calculate eye landmarks and estimated gaze.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Personal calibration in deep learning. The method usually samples a few images from the target domain as calibration samples. The calibration samples and training set are jointly used to improve the performance in target domain.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Different cameras and their captured images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Different platforms and their characteristics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. We illustrate the relation between gaze directions and PoG. Gaze directions are originated from a origin o and intersect with the screen at the PoG. The PoG is usually denoted as a 2D coordinate p. It can be converted to 3D coordinate t in CCS with screen pose Rs and Ts. The gaze direction can also be computed with k(t − o), where k is a scale factor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 15 .</head><label>15</label><figDesc>Fig. 15. Distribution of head psoe and gaze in different datasets. The first row is the distribution of gaze and the second row is the distribution of head.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I SUMMARY</head><label>I</label><figDesc>OF GAZE ESTIMATION METHODS.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II SUMMARY</head><label>II</label><figDesc>OF FACE ALIGNMENT METHODS</figDesc><table><row><cell>Names</cell><cell>Years Pub.</cell><cell>Links</cell></row><row><cell>Dlib [131]</cell><cell>CVPR</cell><cell>https://pypi.org/project/dlib/19.6.0/</cell></row><row><cell>MTCNN [132]</cell><cell>SPL</cell><cell>https://github.com/kpzhang93/M</cell></row><row><cell></cell><cell></cell><cell>TCNN face detection alignment</cell></row><row><cell>DAN [133]</cell><cell cols="2">CVPRW https://github.com/MarekKowalski</cell></row><row><cell></cell><cell></cell><cell>/DeepAlignmentNetwork</cell></row><row><cell>OpenFace [134]</cell><cell>FG</cell><cell>https://github.com/TadasBaltrusait</cell></row><row><cell></cell><cell></cell><cell>is/OpenFace</cell></row><row><cell>PRN [135]</cell><cell>ECCV</cell><cell>https://github.com/YadiraF/PRNet</cell></row><row><cell cols="2">3DDFA V2 [136] ECCV</cell><cell>https://github.com/cleardusk/3DD</cell></row><row><cell></cell><cell></cell><cell>FA V2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III SYMBOL</head><label>III</label><figDesc>TABLE IN DATA POST-PROCESSING</figDesc><table><row><cell>Symbol</cell><cell>meaning</cell></row><row><cell>p ∈ R 2</cell><cell>p = (u, v), gaze targets.</cell></row><row><cell>g ∈ R 3</cell><cell>g = (gx, gy, gz), gaze directions.</cell></row><row><cell>o ∈ R 3</cell><cell>o = (xo, yo, zo), origins of gaze directions.</cell></row><row><cell>t ∈ R 3</cell><cell>t = (xt, yt, zt), targets of gaze directions.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IV SUMMARY</head><label>IV</label><figDesc>OF COMMON GAZE ESTIMATION DATASETS.</figDesc><table><row><cell>Datasets</cell><cell>Subjects</cell><cell>Total</cell><cell cols="3">Annotations Full 2D 3D</cell><cell>Brief Introduction</cell><cell>Links</cell></row><row><cell></cell><cell></cell><cell></cell><cell>face</cell><cell>Gaze</cell><cell>Gaze</cell></row><row><cell>Columbia [140], 2013, **</cell><cell>58</cell><cell>6K images</cell><cell></cell><cell>×</cell><cell></cell><cell>Collected in laboratory; 5 head</cell><cell>https://cs.col</cell></row><row><cell>(Columbia University)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>pose and 21 gaze directions per head pose.</cell><cell>umbia.edu/CA VE/databases/c olumbia gaze</cell></row><row><cell>UTMultiview [30], 2014,</cell><cell>50</cell><cell>1.1M images</cell><cell>×</cell><cell></cell><cell></cell><cell>Collected in laboratory; Fixed</cell><cell>https://ut-visio</cell></row><row><cell>(The University of Tokyo;</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>head pose; Multiview eye images;</cell><cell>n.org/datasets</cell></row><row><cell>Microsoft Research Asia)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Synthesis eye images.</cell></row><row><cell>EyeDiap [145], 2014, ***</cell><cell>16</cell><cell>videos</cell><cell></cell><cell></cell><cell></cell><cell>Collected in laboratory; Free head</cell><cell>https:</cell></row><row><cell>(Idiap Research Institute)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>poes; Additional depth videos.</cell><cell>//idiap.ch/d ataset/eyediap</cell></row><row><cell>MPIIGaze [42], 2015, ***</cell><cell></cell><cell>213K images</cell><cell>×</cell><cell></cell><cell></cell><cell>Collected by laptops in daily life;</cell><cell>https:</cell></row><row><cell>(Max Planck Institute)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Free head pose and illumination.</cell><cell>//mpi-inf.m pg.de/mpiigaze</cell></row><row><cell>GazeCapture [35], 2016,*</cell><cell>1, 474</cell><cell>2.4M images</cell><cell></cell><cell></cell><cell>×</cell><cell>Collected by mobile devices in</cell><cell>https:</cell></row><row><cell>(University of Georgia; MIT; Max Planck Institute)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>daily life; Variable lighting con-dition and head motion.</cell><cell>//gazecapture. csail.mit.edu</cell></row><row><cell>MPIIFaceGaze [43], ***</cell><cell></cell><cell>∼ 45K images</cell><cell></cell><cell></cell><cell></cell><cell>Collected by laptops in daily life;</cell></row><row><cell>2017, (Max Planck Insti-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Free head pose and illumination. footnote 1</cell></row><row><cell>tute)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>InvisibleEye [119], 2017,</cell><cell></cell><cell>280K Images</cell><cell>×</cell><cell></cell><cell>×</cell><cell>Collected in laboratory; Multiple</cell><cell>https:</cell></row><row><cell>(Max Planck Institute; Osaka University)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>near-eye camera; Low resolution cameras .</cell><cell>//mpi-inf.mpg .de/invisibleeye</cell></row><row><cell>TabletGaze [146], 2017,*</cell><cell></cell><cell>videos</cell><cell></cell><cell></cell><cell>×</cell><cell>Collected by tablets in laboratory;</cell><cell>https://sh.rice.</cell></row><row><cell>(Rice University)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Four postures to hold the tablets; Free head pose.</cell><cell>edu/cognitive-e ngagement/tabl etgaze</cell></row><row><cell>RT-Gene [47], 2018,****</cell><cell></cell><cell>123K images</cell><cell></cell><cell>×</cell><cell></cell><cell>Collected in laboratory; Free head</cell><cell>https://github.c</cell></row><row><cell>(Imperial College London)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>pose; Annotated with mobile eye-tracker; Use GAN to remove the</cell><cell>om/Tobias-Fis cher/rt gene</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>eye-tracker in face images.</cell></row><row><cell>Gaze360 [36], 2019, ****</cell><cell></cell><cell>172K images</cell><cell></cell><cell>×</cell><cell></cell><cell>Collected in indoor and outdoor</cell><cell>https://gaze36</cell></row><row><cell>(MIT; Toyota Research Insti-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>environments; A wide range of</cell><cell>0.csail.mit.edu</cell></row><row><cell>tute)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>head poses and distances between</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>subjects and cameras.</cell></row><row><cell>NVGaze [121], 2019, ***</cell><cell></cell><cell>4.5M images</cell><cell>×</cell><cell></cell><cell>×</cell><cell>Collected in laboratory; Near-eye</cell><cell>https://sites.go</cell></row><row><cell>(NVIDIA; UNC)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Images; Infrared illumination.</cell><cell>ogle.com/nvidi a.com/nvgaze</cell></row><row><cell>ShanghaiTechGaze [98],*</cell><cell></cell><cell>224K images</cell><cell></cell><cell></cell><cell>×</cell><cell>Collected in laboratory; Free head</cell><cell>https:</cell></row><row><cell>2019, (ShanghaiTech Uni-versity; UESTC)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>poes; Multiview gaze dataset.</cell><cell>//github.com/d ongzelian/mult i-view-gaze</cell></row><row><cell>ETH-XGaze [82], 2020, *</cell><cell></cell><cell>1.1M images</cell><cell></cell><cell></cell><cell></cell><cell>Collected in laboratory; High-</cell><cell>https:</cell></row><row><cell>(ETH Zurich; Google)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>resolution images; Extreme head pose; 16 illumination conditions.</cell><cell>//ait.ethz.c h/projects/202 0/ETH-XGaze</cell></row><row><cell>EVE [125], 2020,******</cell><cell></cell><cell>∼ 4.2K videos</cell><cell></cell><cell></cell><cell></cell><cell>Collected in laboratory; Free head</cell><cell>https://ait.ethz</cell></row><row><cell>(ETH Zurich)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>pose; Free view; Annotated with desktop eye tracker; Pupil size</cell><cell>.ch/projects/2 020/EVE/</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>annotation.</cell></row></table><note>https://mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/gaze-based-human-computer-interaction/its-written-all-over-your-face-ful l-face-appearance-based-gaze-estimation</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V BENCHMARK</head><label>V</label><figDesc>OF 3D GAZE ESTIMATION.</figDesc><table><row><cell></cell><cell cols="3">Task A: Estimate gaze directions</cell><cell cols="4">Task B: Estimate gaze directions</cell></row><row><cell>Datasets</cell><cell cols="3">originating from eye center</cell><cell cols="3">originating from face center</cell><cell></cell></row><row><cell>Methods</cell><cell>MPIIGaze [42]</cell><cell>EyeDiap [145]</cell><cell>UT [30]</cell><cell>MPIIFaceGaze [43]</cell><cell>EyeDiap [145]</cell><cell>Gaze360 [36]</cell><cell>ETH-XGaze [82]</cell></row><row><cell>Proposed for task A</cell><cell cols="3">Direct results of task A</cell><cell cols="4">Converted results from task A</cell></row><row><cell>Mnist [26]</cell><cell>6.27 •</cell><cell>7.60 •</cell><cell>6.34 •</cell><cell>6.39 •</cell><cell>7.37 •</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>GazeNet [42]</cell><cell>5.70 •</cell><cell>7.13 •</cell><cell>6.44 •</cell><cell>5.76 •</cell><cell>6.79 •</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>Proposed for task B</cell><cell cols="3">Converted results from task B</cell><cell></cell><cell cols="2">Direct results of task B</cell><cell></cell></row><row><cell>Dilated-Net [46]</cell><cell>4.39 •</cell><cell>6.57 •</cell><cell>N/A</cell><cell>4.42 •</cell><cell>6.19 •</cell><cell>13.73 •</cell><cell>N/A</cell></row><row><cell>Gaze360 [36]</cell><cell>4.07 •</cell><cell>5.58 •</cell><cell>N/A</cell><cell>4.06 •</cell><cell>5.36 •</cell><cell>11.04 •</cell><cell>4.46 •</cell></row><row><cell>RT-Gene [47]</cell><cell>4.61 •</cell><cell>6.30 •</cell><cell>N/A</cell><cell>4.66 •</cell><cell>6.02 •</cell><cell>12.26 •</cell><cell>N/A</cell></row><row><cell>FullFace [43]</cell><cell>4.96 •</cell><cell>6.76 •</cell><cell>N/A</cell><cell>4.93 •</cell><cell>6.53 •</cell><cell>14.99 •</cell><cell>7.38 •</cell></row><row><cell>CA-Net [49]</cell><cell>4.27 •</cell><cell>5.63 •</cell><cell>N/A</cell><cell>4.27 •</cell><cell>5.27 •</cell><cell>11.20 •</cell><cell>N/A</cell></row><row><cell>Proposed for task C</cell><cell cols="3">Converted results from task C</cell><cell cols="4">Converted results from task C</cell></row><row><cell>(In Tab. VI)</cell><cell></cell><cell>(In Tab. VI)</cell><cell></cell><cell></cell><cell cols="2">(In Tab. VI)</cell><cell></cell></row><row><cell>Itracker [147]</cell><cell>7.25 •</cell><cell>7.50 •</cell><cell>N/A</cell><cell>7.33 •</cell><cell>7.13 •</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>AFF-Net [50]</cell><cell>3.69 •</cell><cell>6.75 •</cell><cell>N/A</cell><cell>3.73 •</cell><cell>6.41 •</cell><cell>N/A</cell><cell>N/A</cell></row></table><note>*  We will continue to add new methods and datasets. Please keep track of our website for the latest progress.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VI BENCHMARK</head><label>VI</label><figDesc>OF 2D GAZE ESTIMATION. We will continue to add new methods and datasets. Please keep track of our website for the latest progress.</figDesc><table><row><cell>Methods</cell><cell>Datasets</cell><cell cols="2">MPIIFaceGaze [43]</cell><cell cols="4">Task C: Estimate 2D PoG. EyeDiap [145] Tablet GazeCapture [35] Phone</cell></row><row><cell cols="2">Proposed for task C</cell><cell></cell><cell></cell><cell cols="2">Direct results of task C</cell><cell></cell></row><row><cell cols="2">Itracker [35]</cell><cell>7.67 cm</cell><cell></cell><cell></cell><cell>10.13 cm</cell><cell>2.81 cm</cell><cell>1.86 cm</cell></row><row><cell cols="2">AFF-Net [50]</cell><cell>4.21 cm</cell><cell></cell><cell></cell><cell>9.25 cm</cell><cell>2.30 cm</cell><cell>1.62 cm</cell></row><row><cell cols="2">SAGE [111]</cell><cell>N/A</cell><cell></cell><cell></cell><cell>N/A</cell><cell>2.72 cm</cell><cell>1.78 cm</cell></row><row><cell cols="2">TAT [122]</cell><cell>N/A</cell><cell></cell><cell></cell><cell>N/A</cell><cell>2.66 cm</cell><cell>1.77 cm</cell></row><row><cell cols="2">Proposed for task A</cell><cell></cell><cell cols="4">Converted results from task A (In Tab. V)</cell></row><row><cell cols="2">Mnist [26]</cell><cell>7.29 cm</cell><cell></cell><cell></cell><cell>9.06 cm</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell cols="2">GazeNet [42]</cell><cell>6.62 cm</cell><cell></cell><cell></cell><cell>8.51 cm</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell cols="2">Proposed for task B</cell><cell></cell><cell cols="4">Converted results from task B (In Tab. V)</cell></row><row><cell cols="2">Dilated-Net [46]</cell><cell>5.07 cm</cell><cell></cell><cell></cell><cell>7.36 cm</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell cols="2">Gaze360 [36]</cell><cell>4.66 cm</cell><cell></cell><cell></cell><cell>6.37 cm</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell cols="2">RT-Gene [47]</cell><cell>5.36 cm</cell><cell></cell><cell></cell><cell>7.19 cm</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell cols="2">FullFace [43]</cell><cell>5.65 cm</cell><cell></cell><cell></cell><cell>7.70 cm</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell cols="2">CA-Net [49]</cell><cell>4.90 cm</cell><cell></cell><cell></cell><cell>6.30 cm</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>ETH-XGaze</cell><cell>EyeDiap</cell><cell>MPIIGaze</cell><cell cols="2">R T -GENE</cell><cell>UT-Multiview</cell><cell>EVE</cell><cell>Gaze360</cell></row></table><note>*</note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Model Supervised CNN [26]</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Beyond eye gaze: What else can eyetracking reveal about cognition and cognitive development?</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Eckstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Guerra-Carrillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Miller Singley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Bunge</surname></persName>
		</author>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S1878929316300846" />
	</analytic>
	<monogr>
		<title level="j">Developmental Cognitive Neuroscience</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="69" to="91" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Using eye gaze data and visual activities to infer human cognitive styles: Method and feasibility studies</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Raptis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Katsini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Belk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fidas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Samaras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Avouris</surname></persName>
		</author>
		<idno type="DOI">10.1145/3079628.3079690</idno>
		<ptr target="https://doi.org/10.1145/3079628.3079690" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th Conference on User Modeling, Adaptation and Personalization, ser. UMAP &apos;17</title>
				<meeting>the 25th Conference on User Modeling, Adaptation and Personalization, ser. UMAP &apos;17<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="164" to="173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The promise of eye-tracking methodology in organizational research: A taxonomy, review, and future avenues</title>
		<author>
			<persName><forename type="first">M</forename><surname>Meißner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Oll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Organizational Research Methods</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="590" to="617" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Eye-tracking research in eating disorders: A systematic review</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kerr-Gaffney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tchanturia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Eating Disorders</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="27" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Evaluation of appearancebased methods and implications for gaze-based applications</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bulling</surname></persName>
		</author>
		<idno type="DOI">10.1145/3290605.3300646</idno>
		<ptr target="https://doi.org/10.1145/3290605.3300646" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems, ser. CHI &apos;19</title>
				<meeting>the 2019 CHI Conference on Human Factors in Computing Systems, ser. CHI &apos;19<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Appearancebased gaze estimator for natural interaction control of surgical robots</title>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yip</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="25" to="095" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hybrid gaze/eeg brain computer interface for robot arm control on a pick and place task</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1476" to="1479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Predicting the driver&apos;s focus of attention: The dr(eye)ve project</title>
		<author>
			<persName><forename type="first">A</forename><surname>Palazzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Abati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Calderara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Solera</surname></persName>
		</author>
		<author>
			<persName><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1720" to="1733" />
			<date type="published" when="2019-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Saliency in vr: How do people explore virtual environments?</title>
		<author>
			<persName><forename type="first">V</forename><surname>Sitzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Serrano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pavel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Agrawala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gutierrez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Masia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wetzstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1633" to="1642" />
			<date type="published" when="2018-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Slam-based localization of 3d gaze using a mobile eye tracker</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 ACM Symposium on Eye Tracking Research &amp; Applications</title>
				<meeting>the 2018 ACM Symposium on Eye Tracking Research &amp; Applications</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Survey of eye movement recording methods</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sheena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Behavior research methods &amp; instrumentation</title>
				<imprint>
			<date type="published" when="1975" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="397" to="429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Eye movement recordings: methods</title>
		<author>
			<persName><forename type="first">T</forename><surname>Eggert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuro-Ophthalmology</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="15" to="34" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adaptive linear regression for appearance-based gaze estimation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Okabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2033" to="2046" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Gaze estimation using local features and non-linear regression</title>
		<author>
			<persName><forename type="first">F</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Carbone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pissaloux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1961" to="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Appearance-based eye gaze estimation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Kar-Han Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Applications of Computer Vision (WACV)</title>
				<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="191" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sparse and semi-supervised visual mapping with the s 3 gp</title>
		<author>
			<persName><forename type="first">O</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The corneo-retinal potential difference as the basis of the galvanometric method of recording eye movements</title>
		<author>
			<persName><forename type="first">O</forename><surname>Mowrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Ruch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Journal of Physiology-Legacy Content</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="423" to="428" />
			<date type="published" when="1936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Uber die registrierung des nystagmus und anderer augenbewegungen verm itteles des saitengalvanometers</title>
		<author>
			<persName><forename type="first">E</forename><surname>Schott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Deut Arch fur klin Med</title>
		<imprint>
			<biblScope unit="volume">140</biblScope>
			<biblScope unit="page" from="79" to="90" />
			<date type="published" when="1922" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Eye gaze tracking techniques for interactive applications</title>
		<author>
			<persName><forename type="first">C</forename><surname>Morimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mimica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="24" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Heuristic filtering and reliable calibration methods for video-based pupil-tracking systems</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Stampe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods, Instruments, &amp; Computers</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="142" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Real-time eye, gaze, and face pose tracking for monitoring driver vigilance</title>
		<author>
			<persName><forename type="first">J</forename><surname>Qiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Real-Time Imaging</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="357" to="377" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">General theory of remote gaze estimation using the pupil center and corneal reflections</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Eizenman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1124" to="1133" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Novel eye gaze tracking techniques under natural head movement</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2246" to="2260" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Combining head pose and eye location information for gaze estimation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Valenti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="802" to="815" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Geometric generative gaze estimation (g3e) for remote rgb-d cameras</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Mora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Odobez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1773" to="1780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Appearance-based gaze estimation in the wild</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bulling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Non-intrusive gaze tracking using artificial neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Baluja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pomerleau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">USA, Tech. Rep</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Appearance-based gaze estimation using visual saliency</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Matsushita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="329" to="341" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Person independent 3d gaze estimation from remote rgb-d cameras</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Mora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Odobez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
				<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2791" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning-by-synthesis for appearance-based 3d gaze estimation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Matsushita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Person-independent eye gaze prediction from eye images using patch-based features</title>
		<author>
			<persName><forename type="first">F</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S092523121501783X" />
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">182</biblScope>
			<biblScope unit="page" from="10" to="17" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">An incremental learning method for unconstrained gaze estimation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Matsushita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Koike</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-540-88690-7_49</idno>
		<ptr target="https://doi.org/10.1007/978-3-540-88690-7" />
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision, ser. ECCV &apos;08</title>
				<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="656" to="667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning gaze biases with head motion for head pose-free gaze estimation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Okabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S0262885614000171" />
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="169" to="179" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Gaze estimation from eye appearance: A head pose-free method via eye image synthesis</title>
		<author>
			<persName><forename type="first">F</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Okabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3680" to="3693" />
			<date type="published" when="2015-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Eye tracking for everyone</title>
		<author>
			<persName><forename type="first">K</forename><surname>Krafka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kellnhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhandarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Matusik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Gaze360: Physically unconstrained gaze estimation in the wild</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kellnhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Stent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Matusik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Gaze estimation by exploring two-eye asymmetry</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="5259" to="5272" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep pictorial gaze estimation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Spurr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Hilliges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
				<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Improving few-shot user-specific gaze adaptation via gaze redirection synthesis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Odobez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Few-shot adaptive gaze estimation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Mello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Hilliges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Vulnerability of appearance-based gaze estimation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.13134</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Mpiigaze: Realworld dataset and deep appearance-based gaze estimation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bulling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="162" to="175" />
			<date type="published" when="2019-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">It&apos;s written all over your face: Full-face appearance-based gaze estimation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bulling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
				<imprint>
			<publisher>CVPRW). IEEE</publisher>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="2299" to="2308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A novel approach to real-time non-intrusive gaze finding</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Machin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sheppard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
				<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="428" to="437" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Appearance-based gaze estimation using dilated-convolutions</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ACCV 2018</title>
				<editor>
			<persName><forename type="first">C</forename><surname>Jawahar</surname></persName>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
			<persName><forename type="first">G</forename><surname>Mori</surname></persName>
			<persName><forename type="first">K</forename><surname>Schindler</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="309" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Rt-gene: Real-time eye gaze estimation in natural environments</title>
		<author>
			<persName><forename type="first">T</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Jin</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Demiris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
				<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Appearance-based gaze estimation via evaluation-guided asymmetric regression</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
				<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A coarseto-fine adaptive network for appearance-based gaze estimation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
				<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Adaptive feature fusion network for gaze tracking in mobile tablets</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Pattern Recognition (ICPR)</title>
				<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Generalizing eye tracking with bayesian adversarial learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Gaze estimation in the dark with generative adversarial networks</title>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-W</forename><surname>Jeong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 ACM Symposium on Eye Tracking Research &amp; Applications</title>
				<meeting>the 2020 ACM Symposium on Eye Tracking Research &amp; Applications</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="3" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Driver gaze estimation in the real world: Overcoming the eyeglass challenge</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rangesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Intelligent Vehicles Symposium (IV)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1054" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning for gaze estimation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Odobez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Remote gaze estimation with a single camera based on facial-feature tracking without special calibration actions</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yamazoe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Utsumi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yonezawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Abe</surname></persName>
		</author>
		<idno type="DOI">10.1145/1344471.1344527</idno>
		<ptr target="https://doi.org/10.1145/1344471.1344527" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM Symposium on Eye Tracking Research &amp; Applications, ser. ETRA &apos;08</title>
				<meeting>the 2008 ACM Symposium on Eye Tracking Research &amp; Applications, ser. ETRA &apos;08<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="245" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">3d gaze estimation with a single camera without ir illumination</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition (ICPR)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Person-independent 3d gaze estimation using face frontalization</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Jeni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition Workshop</title>
				<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="87" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Lpm: Learnable pooling module for efficient full-face gaze estimation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ogusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yamanaka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 14th IEEE International Conference on Automatic Face Gesture Recognition (FG 2019)</title>
				<imprint>
			<date type="published" when="2019-05" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Learning-based region selection for end-to-end gaze estimation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bulling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Hilliges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The British Machine Vision Conference (BMVC)</title>
				<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Puregaze: Purifying gaze feature for generalizable gaze estimation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.13173</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">A multi-modal approach for driver gaze prediction to remove identity bias</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Multimodal Interaction</title>
				<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="768" to="776" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Driver&apos;s gaze zone estimation method: A four-channel convolutional neural network model</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Big-data Service and Intelligent Computation</title>
				<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="20" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Learning to detect head movement in unconstrained remote gaze estimation in the wild</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Winter Conference on Applications of Computer Vision</title>
				<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3443" to="3452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Monocular free-head 3d gaze tracking with deep learning and geometry constraints</title>
		<author>
			<persName><forename type="first">H</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">On the properties of neural machine translation: Encoder-decoder approaches</title>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Recurrent cnn for 3d gaze estimation using appearance and shape cues</title>
		<author>
			<persName><forename type="first">C</forename><surname>Palmero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Selva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Bagheri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The British Machine Vision Conference (BMVC)</title>
				<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Gaze estimation for assisted living environments</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Dias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Malafronte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Medeiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Odone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
				<imprint>
			<date type="published" when="2020-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Automatic eye gaze estimation using geometric &amp; texture-based networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jyoti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 24th International Conference on Pattern Recognition (ICPR)</title>
				<imprint>
			<date type="published" when="2018-08" />
			<biblScope unit="page" from="2474" to="2479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Unsupervised learning of eye gaze representation from the web</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Joint Conference on Neural Networks (IJCNN)</title>
				<imprint>
			<date type="published" when="2019-07" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Learning a 3d gaze estimator with improved itracker combined with bidirectional lstm</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo (ICME)</title>
				<imprint>
			<date type="published" when="2019-07" />
			<biblScope unit="page" from="850" to="855" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Realtime and accurate 3d eye gaze capture with dcnn-based iris and pupil segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Neuro-inspired eye tracking with eye movement dynamics</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Dynamic bayesian networks: representation, inference and learning</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Russell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Learning hierarchical features for scene labeling</title>
		<author>
			<persName><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1915" to="1929" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-assisted Intervention</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Gaze estimation with multi-scale channel and spatial attention</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Computing and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="303" to="309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Gaze-net: appearance-based gaze estimation using capsule networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mahanama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jayawardana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jayarathna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Augmented Human International Conference</title>
				<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Convolutional neural network implementation for eye-gaze estimation on low-quality consumer imaging systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lemley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Drimbarean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Corcoran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Consumer Electronics</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="187" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Ethxgaze: A large scale dataset for gaze estimation under extreme head pose and gaze variation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Beeler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Hilliges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
				<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Appearance-based gaze estimation using separable convolution neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Advanced Information Technology, Electronic and Automation Control Conference (IAEAC)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="609" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Deep learning based eye gaze tracking for automotive applications: An auto-keras approach</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bublea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Cȃleanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Symposium on Electronics and Telecommunications (ISETC)</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Look me in the eyes: A survey of eye and gaze animation for virtual agents and artificial systems</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ruhland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Andrist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Badler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Badler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gleicher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mcdonnell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eurographics</title>
		<imprint>
			<biblScope unit="page" from="69" to="91" />
			<date type="published" when="2014-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Rendering synthetic ground truth images for eye tracker evaluation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Swirski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dodgson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 ACM Symposium on Eye Tracking Research &amp; Applications, ser. ETRA &apos;14</title>
				<meeting>the 2014 ACM Symposium on Eye Tracking Research &amp; Applications, ser. ETRA &apos;14</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="219" to="222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">U2eyes: A binocular dataset for eye tracking and gaze estimation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Porta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bossavit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cabeza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Larumbe-Bergera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Garde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Villanueva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)</title>
				<imprint>
			<date type="published" when="2019-10" />
			<biblScope unit="page" from="3660" to="3664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Accurate, dense, and robust multiview stereopsis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Furukawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1362" to="1376" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Rendering of eyes for eye-shape registration and gaze estimation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Baltrusaitis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bulling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Learning an appearance-based gaze estimator from one million synthesised images</title>
		<author>
			<persName><forename type="first">E</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Baltrušaitis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bulling</surname></persName>
		</author>
		<idno type="DOI">10.1145/2857491.2857492</idno>
		<ptr target="https://doi.org/10.1145/2857491.2857492" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM Symposium on Eye Tracking Research &amp; Applications, ser. ETRA &apos;16</title>
				<meeting>the 2016 ACM Symposium on Eye Tracking Research &amp; Applications, ser. ETRA &apos;16<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="131" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Learning from simulated and unsupervised images through adversarial training</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Susskind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Webb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Mixed effects neural networks (menets) with applications to gaze estimation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">Variational algorithms for approximate bayesian inference</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Beal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
		<respStmt>
			<orgName>UCL (University College London</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<title level="m" type="main">The annals of mathematical statistics</title>
		<author>
			<persName><forename type="first">H</forename><surname>Robbins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Monro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1951" />
			<biblScope unit="page" from="400" to="407" />
		</imprint>
	</monogr>
	<note>A stochastic approximation method</note>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Deep multitask gaze estimation with a constrained landmark-gaze model</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Odobez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision Workshop (ECCVW)</title>
				<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Multitask learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="75" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<title level="m" type="main">An overview of multi-task learning in deep neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Multiview multitask gaze estimation with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3010" to="3023" />
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Rgbd based gaze estimation via multi-task cnn</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="2488" to="2495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Eyenet: A multi-task deep network for off-axis eye gaze estimation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rajendran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Van As</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)</title>
				<imprint>
			<date type="published" when="2019-10" />
			<biblScope unit="page" from="3683" to="3687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Following gaze in video</title>
		<author>
			<persName><forename type="first">A</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Saliency unified: A deep architecture for simultaneous eye fixation prediction and salient object segmentation</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Kruthiventi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gudisa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Dholakiya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5781" to="5790" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Inferring salient objects from human fixations</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1913" to="1927" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Connecting gaze, scene, and attention: Generalized attention estimation via joint modeling of gaze and scene saliency</title>
		<author>
			<persName><forename type="first">E</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rozga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
				<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Bidirectional lstm networks for improved phoneme classification and recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Artificial Neural Networks, ser. ICANN&apos;05</title>
				<meeting>International Conference on Artificial Neural Networks, ser. ICANN&apos;05</meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="799" to="804" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Offset calibration for appearance-based gaze estimation via gaze decomposition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
				<imprint>
			<date type="published" when="2020-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Training person-specific gaze estimators from user interactions with multiple devices</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bulling</surname></persName>
		</author>
		<idno type="DOI">10.1145/3173574.3174198</idno>
		<ptr target="https://doi.org/10.1145/3173574.3174198" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CHI Conference on Human Factors in Computing Systems, ser. CHI &apos;18</title>
				<meeting>the CHI Conference on Human Factors in Computing Systems, ser. CHI &apos;18<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Evaluation of appearance-based eye tracking calibration data selection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Artificial Intelligence and Computer Applications (ICAICA)</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Learning to personalize in appearance-based gaze tracking</title>
		<author>
			<persName><forename type="first">E</forename><surname>Lindén</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sjöstrand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Proutiere</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)</title>
				<imprint>
			<date type="published" when="2019-10" />
			<biblScope unit="page" from="1140" to="1148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">On-device few-shot personalization for real-time gaze estimation</title>
		<author>
			<persName><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Valliappan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lagun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Navalpakkam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)</title>
				<imprint>
			<date type="published" when="2019-10" />
			<biblScope unit="page" from="1149" to="1158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Domain adaptation gaze estimation by embedding with prediction consistency</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Asian Conference on Computer Vision</title>
				<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Specialized gaze estimation for children by convolutional neural network and domain adaptation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing (ICIP)</title>
				<imprint>
			<date type="published" when="2017-09" />
			<biblScope unit="page" from="3305" to="3309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Geodesic flow kernel for unsupervised domain adaptation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2066" to="2073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">A differential approach for gaze estimation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Mora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Odobez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Salgaze: Personalizing gaze estimation using visual saliency</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Di Martino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Espinosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)</title>
				<imprint>
			<date type="published" when="2019-10" />
			<biblScope unit="page" from="1169" to="1178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Deep eye fixation map learning for calibration-free eye gaze tracking</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
		<idno type="DOI">10.1145/2857491.2857515</idno>
		<ptr target="https://doi.org/10.1145/2857491.2857515" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM Symposium on Eye Tracking Research &amp; Applications, ser. ETRA &apos;16</title>
				<meeting>the 2016 ACM Symposium on Eye Tracking Research &amp; Applications, ser. ETRA &apos;16<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="47" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Self-adaptive appearance-based eye-tracking with online transfer learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Klein Salvalaio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>De Oliveira Ramos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 8th Brazilian Conference on Intelligent Systems (BRACIS)</title>
				<imprint>
			<date type="published" when="2019-10" />
			<biblScope unit="page" from="383" to="388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Invisibleeye: Mobile eye tracking using multiple low-resolution cameras and learning-based gaze estimation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tonsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Steil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bulling</surname></persName>
		</author>
		<idno type="DOI">10.1145/3130971</idno>
		<ptr target="https://doi.org/10.1145/3130971" />
	</analytic>
	<monogr>
		<title level="j">Proc. ACM Interact. Mob. Wearable Ubiquitous Technol</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2017-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Eye tracking in augmented spaces: A deep learning approach</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lemley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Corcoran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Games, Entertainment, Media Conference (GEM)</title>
				<imprint>
			<date type="published" when="2018-08" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Nvgaze: An anatomically-informed dataset for low-latency, near-eye gaze estimation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stengel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Majercik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">De</forename><surname>Mello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dunn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mcguire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Luebke</surname></persName>
		</author>
		<idno type="DOI">10.1145/3290605.3300780</idno>
		<ptr target="https://doi.org/10.1145/3290605.3300780" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems, ser. CHI &apos;19</title>
				<meeting>the 2019 CHI Conference on Human Factors in Computing Systems, ser. CHI &apos;19<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">A generalized and robust method towards practical gaze estimation on smart phone</title>
		<author>
			<persName><forename type="first">T</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">I</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)</title>
				<imprint>
			<date type="published" when="2019-10" />
			<biblScope unit="page" from="1131" to="1139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Estimating a driver&apos;s gaze point by a remote spherical camera</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kosaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Mechatronics and Automation (ICMA)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="599" to="604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Gaze estimation based on deep learning method</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Computer Science and Application Engineering</title>
				<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Towards end-to-end video-based eye-tracking</title>
		<author>
			<persName><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Aksan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Hilliges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="747" to="763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Quantifying the qualities of aesthetics in product design using eye-tracking technology</title>
		<author>
			<persName><forename type="first">S</forename><surname>Khalighy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Scheepers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Whittet</surname></persName>
		</author>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S0169814115000761" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Industrial Ergonomics</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="31" to="43" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Eye tracking in neuromarketing: a research agenda for marketing studies</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D O J</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H C</forename><surname>De Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Rocha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D M E</forename><surname>Giraldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of psychological studies</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">32</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Everyday eye contact detection using unsupervised gaze target discovery</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bulling</surname></persName>
		</author>
		<idno type="DOI">10.1145/3126594.3126614</idno>
		<ptr target="https://doi.org/10.1145/3126594.3126614" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology, ser. UIST &apos;17</title>
				<meeting>the 30th Annual ACM Symposium on User Interface Software and Technology, ser. UIST &apos;17<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="193" to="203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Aggregaze: Collective estimation of audience attention on public displays</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bulling</surname></persName>
		</author>
		<idno type="DOI">10.1145/2984511.2984536</idno>
		<ptr target="https://doi.org/10.1145/2984511.2984536" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th Annual ACM Symposium on User Interface Software and Technology, ser. UIST &apos;16</title>
				<meeting>the 29th Annual ACM Symposium on User Interface Software and Technology, ser. UIST &apos;16<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="821" to="831" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Exploiting surroundedness for saliency detection: A boolean map approach</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="889" to="902" />
			<date type="published" when="2016-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">One millisecond face alignment with an ensemble of regression trees</title>
		<author>
			<persName><forename type="first">V</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1867" to="1874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Joint face detection and alignment using multitask cascaded convolutional networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1499" to="1503" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Deep alignment network: A convolutional neural network for robust face alignment</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kowalski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Naruniec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Trzcinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
				<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Openface 2.0: Facial behavior analysis toolkit</title>
		<author>
			<persName><forename type="first">T</forename><surname>Baltrusaitis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">C</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th IEEE International Conference on Automatic Face Gesture Recognition (FG 2018)</title>
				<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="59" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">Joint 3d face reconstruction and dense alignment with position map regression network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
				<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">Towards fast, accurate and stable 3d dense face alignment</title>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
				<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Robust facial landmark detection via heatmap-offset regression</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="5050" to="5064" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">Attention-driven cropping for very high resolution facial landmark detection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Chandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Beeler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">A coarse-to-fine facial landmark detection method based on self-attention mechanism</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">Gaze locking: Passive eye contact detection for human-object interaction</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Feiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual ACM Symposium on User Interface Software and Technology, ser. UIST &apos;13</title>
				<meeting>the 26th Annual ACM Symposium on User Interface Software and Technology, ser. UIST &apos;13<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="271" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<monogr>
		<title/>
		<idno type="DOI">10.1145/2501988.2501994</idno>
		<ptr target="https://doi.org/10.1145/2501988.2501994" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">Revisiting data normalization for appearance-based gaze estimation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bulling</surname></persName>
		</author>
		<idno type="DOI">10.1145/3204493.3204548</idno>
		<ptr target="https://doi.org/10.1145/3204493.3204548" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 ACM Symposium on Eye Tracking Research &amp; Applications, ser. ETRA &apos;18</title>
				<meeting>the 2018 ACM Symposium on Eye Tracking Research &amp; Applications, ser. ETRA &apos;18<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">Gaze awareness improves collaboration efficiency in a collaborative assembly task</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th ACM Symposium on Eye Tracking Research &amp; Applications</title>
				<meeting>the 11th ACM Symposium on Eye Tracking Research &amp; Applications</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">Hybrid brain computer interface via bayesian integration of eeg and eye gaze</title>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International IEEE/EMBS Conference on Neural Engineering (NER)</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">Gaze estimation using residual neural network</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">T</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Deepu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Pervasive Computing and Communications Workshops (PerCom Workshops)</title>
				<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="411" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">Eyediap: A database for the development and evaluation of gaze estimation algorithms from rgb and rgb-d cameras</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Mora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Monay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Odobez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 ACM Symposium on Eye Tracking Research &amp; Applications</title>
				<meeting>the 2014 ACM Symposium on Eye Tracking Research &amp; Applications</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">Tabletgaze: dataset and analysis for unconstrained appearance-based gaze estimation in mobile tablets</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Veeraraghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sabharwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Vision and Applications</title>
				<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="445" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">Eye tracking for everyone</title>
		<author>
			<persName><forename type="first">K</forename><surname>Krafka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kellnhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhandarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Matusik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2176" to="2184" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>

<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Text Reuse Detection Using a Composition of Text Similarity Measures</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Daniel</forename><surname>Bär</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">) Ubiquitous Knowledge Processing Lab (UKP-TUDA</orgName>
								<orgName type="institution">Technische Universität Darmstadt</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Torsten</forename><surname>Zesch</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">) Ubiquitous Knowledge Processing Lab (UKP-TUDA</orgName>
								<orgName type="institution">Technische Universität Darmstadt</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">) Ubiquitous Knowledge Processing Lab (UKP-DIPF) German Institute for Educational Research and Educational Information</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">) Ubiquitous Knowledge Processing Lab (UKP-TUDA</orgName>
								<orgName type="institution">Technische Universität Darmstadt</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">) Ubiquitous Knowledge Processing Lab (UKP-DIPF) German Institute for Educational Research and Educational Information</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Text Reuse Detection Using a Composition of Text Similarity Measures</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-07-21T08:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>text similarity</term>
					<term>text reuse</term>
					<term>plagiarism</term>
					<term>paraphrase GERMAN: Textähnlichkeit</term>
					<term>Textwiederverwendung</term>
					<term>Plagiat</term>
					<term>Paraphrase</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Detecting text reuse is a fundamental requirement for a variety of tasks and applications, ranging from journalistic text reuse to plagiarism detection. Text reuse is traditionally detected by computing similarity between a source text and a possibly reused text. However, existing text similarity measures exhibit a major limitation: They compute similarity only on features which can be derived from the content of the given texts, thereby inherently implying that any other text characteristics are negligible. In this paper, we overcome this traditional limitation and compute similarity along three characteristic dimensions inherent to texts: content, structure, and style. We explore and discuss possible combinations of measures along these dimensions, and our results demonstrate that the composition consistently outperforms previous approaches on three standard evaluation datasets, and that text reuse detection greatly benefits from incorporating a diverse feature set that reflects a wide variety of text characteristics.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Text reuse is a common phenomenon and arises, for example, on the Web from mirroring texts on different sites or reusing texts in public blogs. In other text collections such as content authoring systems of communities or enterprises, text reuse arises from keeping multiple versions, copies containing customizations or reformulations, or the use of template texts <ref type="bibr" target="#b7">(Broder et al., 1997)</ref>.</p><p>Problems with text reuse particularly arise in settings where systems are extensively used in a collaborative manner. For example, wikis are web-based, collaborative content authoring systems which offer fast and simple means for adding and editing content <ref type="bibr" target="#b35">(Leuf and Cunningham, 2001)</ref>. At any time, users can modify content already present in the wiki, augment existing texts with new facts, ideas, or thoughts, or create new texts from scratch. However, when users contribute to wikis, they need to avoid content duplication. This requires comprehensive knowledge of what content is already present in the wiki, and what is not. As wikis are traditionally growing fast, this is hardly feasible, though. To remedy this issue, we aim at supporting authors of collaborative text collections by means of automatic text reuse detection. We envision a semi-supervised system that informs a content author of potentially pre-existing instances of text reuse, and then lets the author decide how to proceed, e.g. to merge both texts.</p><p>Detecting text reuse has been studied in a variety of tasks and applications, e.g. the detection of journalistic text reuse <ref type="bibr" target="#b13">(Clough et al., 2002)</ref>, the identification of rewrite sources for ancient literary texts <ref type="bibr" target="#b34">(Lee, 2007)</ref>, or the analysis of text reuse in blogs and web pages <ref type="bibr" target="#b0">(Abdel-Hamid et al., 2009)</ref>. Another common instance of text reuse is plagiarism, with the additional constraint that the reuse needs to be unacknowledged. Near-duplicate detection is also a broad field of related work where the detection of text reuse is crucial, e.g. in the context of web search and crawling <ref type="bibr" target="#b26">(Hoad and Zobel, 2003;</ref><ref type="bibr" target="#b25">Henzinger, 2006;</ref><ref type="bibr" target="#b40">Manku et al., 2007)</ref>. Prior work, however, mainly utilizes fingerprinting and hashing techniques <ref type="bibr" target="#b11">(Charikar, 2002)</ref> for text comparison rather than methods from natural language processing.</p><p>A common approach to text reuse detection is to compute similarity between a source text and a possibly reused text. A multitude of text similarity measures have been proposed for computing similarity based on surface-level and/or semantic features <ref type="bibr" target="#b42">(Mihalcea et al., 2006;</ref><ref type="bibr" target="#b32">Landauer et al., 1998;</ref><ref type="bibr" target="#b18">Gabrilovich and Markovitch, 2007)</ref>. However, existing similarity measures typically exhibit a major limitation: They compute similarity only on features which can be derived from the content of the given texts. By following this approach, they inherently imply that the similarity computation process does not need to take any other text characteristics into account.</p><p>In contrast, we propose that text reuse detection indeed benefits from also assessing similarity along other text characteristics (dimensions, henceforth). We follow empirical evidence by <ref type="bibr" target="#b3">Bär et al. (2011)</ref> and focus on three characteristic similarity dimensions inherent to texts: content, structure, and style. Figure <ref type="figure">1</ref> shows an example of text reuse taken from the Wikipedia Rewrite Corpus (see Section 3.1) where parts of a given source text have been reused either verbatim or by using similar words or phrases. As the example illustrates, the process of creating reused text includes a revision step in which the editor has a certain degree of freedom on how to reuse the source text. This kind of similarity is detectable by content-centric text similarity measures. However, the editor has further split the source text into two individual sentences and changed the order of the reused parts. For detecting the degree of similarity of such a revision, text similarity measures for structural similarity are necessary. Additionally, the given texts exhibit a certain degree of similarity with respect to stylistic features, e.g. vocabulary richness. <ref type="bibr">1</ref> In The type-token ratio <ref type="bibr" target="#b53">(Templin, 1957)</ref> of the texts is .79 and .71, respectively. Source Text. PageRank is a link analysis algorithm used by the ::::: Google ::::::: Internet :::::: search ::::: engine that assigns a numerical weighting to :::: each ::::::: element of a ::::::::: hyperlinked ::: set ::: of ::::::::</p><p>documents, such as the World Wide Web, with the purpose of "measuring" its relative importance within the set.</p><p>Text Reuse. The PageRank algorithm is used to designate :::: every ::::: aspect of a :: set ::: of ::::::::: hyperlinked ::::::::: documents with a numerical weighting. It is used by the :::::: Google :::::: search ::::: engine to estimate the relative importance of a web page according to this weighting.</p><p>Figure <ref type="figure">1</ref>: Example of text reuse taken from the Wikipedia Rewrite Corpus <ref type="bibr" target="#b14">(Clough and Stevenson, 2011)</ref>. Various parts of the source text have been reused, either verbatim (underlined) or using similar words or phrases (wavy underlined). However, the editor has split the source text into two individual sentences and changed the order of the reused parts.</p><p>order to use such features as indicators of text reuse, we propose to further include measures of stylistic similarity.</p><p>In this paper, we thus overcome the traditional limitation of text similarity measures to content features. In contrast, we adopt ideas of seminal studies by cognitive scientists <ref type="bibr" target="#b54">(Tversky, 1977;</ref><ref type="bibr" target="#b21">Goodman, 1972;</ref><ref type="bibr" target="#b20">Gärdenfors, 2000)</ref> and discuss the role of three similarity dimensions for the task of text reuse detection: content, structure, and style, as proposed in our previous work <ref type="bibr" target="#b3">(Bär et al., 2011)</ref>. In Section 2, we report on a multitude of text similarity measures from these dimensions that we used for our experiments. In Section 3, we demonstrate empirically that text reuse can be best detected if measures are combined across dimensions, so that a wide variety of text characteristics are taken into consideration. Our approach consistently outperforms previous work on three standard evaluation datasets, and demonstrates the advantage of integrating text characteristics other than content into the similarity computation process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Text Similarity Measures</head><p>In this section, we report on a variety of similarity measures which we used to compute similarity along characteristic dimensions inherent to texts. <ref type="bibr">2</ref> We classify them into measures for content similarity, structural similarity, and stylistic similarity, as proposed by <ref type="bibr" target="#b3">Bär et al. (2011)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Content Similarity</head><p>Probably the easiest way to reuse text is verbatim copying. It can be detected by using string measures which operate on substring sequences. The longest common substring measure <ref type="bibr" target="#b22">(Gusfield, 1997)</ref> compares the length of the longest contiguous sequence of characters between two texts, normalized by the text lengths. However, the editorial process in journalistic text reuse or the attempt to obfuscate copying in plagiarism may shorten the longest common substring considerably, e.g. when words are inserted or deleted, or parts of reused text appear in a different order. The longest common subsequence measure <ref type="bibr" target="#b1">(Allison and Dix, 1986)</ref> drops the contiguity requirement and allows to detect text reuse in case of word insertions/deletions. Greedy String Tiling <ref type="bibr" target="#b56">(Wise, 1996)</ref> further allows to deal with reordered parts of reused text as it determines a set of shared contiguous substrings between two given documents, each substring thereby being a match of maximal length. A multitude of other string similarity measures have been proposed which view texts as sequences of characters and compute their degree of distance according to a given metric. We used the following measures in our experiments: <ref type="bibr" target="#b27">Jaro (1989)</ref>, <ref type="bibr">Jaro-Winkler (Winkler, 1990)</ref>, <ref type="bibr" target="#b43">Monge and</ref><ref type="bibr" target="#b43">Elkan (1997), and</ref><ref type="bibr" target="#b36">Levenshtein (1966)</ref>.</p><p>Starting from the observation that not all words in a document are of equal importance, we further employed a similarity measure which weights all words by a tfidf scheme <ref type="bibr" target="#b47">(Salton and McGill, 1983)</ref> and computes text similarity as the cosine between two document vectors.</p><p>Comparing word n-grams <ref type="bibr" target="#b39">(Lyon et al., 2001</ref>) is a popular means for comparing lexical patterns between two texts. The more similar the patterns, the more likely is it that text reuse has occurred. After compiling two sets of n-grams, we compared them using the Jaccard coefficient, following <ref type="bibr" target="#b39">Lyon et al. (2001)</ref>, as well as using the containment measure <ref type="bibr" target="#b6">(Broder, 1997)</ref>. We tested n-gram sizes for n = 1, 2, . . . , 15, and will use the original system name Ferret <ref type="bibr" target="#b38">(Lyon et al., 2004)</ref> to refer to the variant with n = 3 using the Jaccard coefficient, henceforth.</p><p>Following the idea of comparing lexical patterns, we also used a measure which has not yet been considered for assessing content similarity: character n-gram profiles <ref type="bibr" target="#b29">(Keselj et al., 2003)</ref>. <ref type="bibr">3</ref> We follow the implementation by <ref type="bibr" target="#b4">Barrón-Cedeño et al. (2010)</ref> and discard all characters (case insensitive) which are not in the alphabet Σ = {a, . . . , z, 0, . . . , 9}, then generate all n-grams on character level, weight them by a tfidf scheme, and finally compare the feature vectors of both the rewritten and the source text using the cosine measure. While in the original implementation only n = 3 was used, we generalize the measure to n = 2, 3, . . . , 15.</p><p>In cases where the editor replaced content words by synonyms, string measures typically fail due to the vocabulary gap. We thus used similarity measures which are capable of measuring semantic similarity between words. We used the following word similarity measures with WordNet <ref type="bibr" target="#b16">(Fellbaum, 1998)</ref>: <ref type="bibr" target="#b28">Jiang and Conrath (1997)</ref>, <ref type="bibr" target="#b37">Lin (1998), and</ref><ref type="bibr" target="#b46">Resnik (1995)</ref>. In order to scale these pairwise word similarity scores to the document level, we follow the aggregation strategy by <ref type="bibr" target="#b42">Mihalcea et al. (2006)</ref>: First, a directional similarity score sim d (T i , T j ) is computed from a text T i to a second text T j (Eq. 1). Therefore, for each word w i in T i , its best-matching counterpart in T j is sought (ma xSim(w i , T j )). The similarity scores of all these matches are summed up and weighted according to their inverse document frequency idf (Spärck <ref type="bibr" target="#b51">Jones, 1972)</ref>, then normalized. The final document-level similarity figure is the average of applying this strategy in both directions, from T i to T j and vice-versa (Eq. 2).</p><formula xml:id="formula_0">sim d (T i , T j ) = w i maxSim(w i , T j ) • id f (w i ) w i id f (w i ) (1) sim(T i , T j ) = 1 2 sim d (T i , T j ) + sim d (T j , T i ) (2)</formula><p>We also tested text expansion mechanisms with the semantic word similarity measures described above: We used the Moses SMT system <ref type="bibr" target="#b31">(Koehn et al., 2007)</ref>, trained on Europarl <ref type="bibr" target="#b30">(Koehn, 2005)</ref>, to translate the original English texts via a bridge language (Dutch) back to English. Thereby, the idea was that in the translation process additional lexemes are introduced which alleviate potential lexical gaps. We computed pairwise word similarity with the measures described above and aggregated according to <ref type="bibr" target="#b42">Mihalcea et al. (2006)</ref>.</p><p>Furthermore, we used the statistical technique Latent Semantic Analysis (LSA) <ref type="bibr">(Landauer et al.,</ref> Traditionally, character n-gram profiles have rather been shown successful for authorship attribution. However, the similarity scores of word n-grams and those of character n-gram profiles are highly correlated: Assuming 5 characters per word on average for English texts <ref type="bibr" target="#b50">(Shannon, 1951)</ref>, we set n = 3 for word n-grams and n = 15 for character n-grams, and computed Pearson's correlation r between the corresponding similarity scores. We obtained r = .93 and r = .86 on the datasets introduced in Sections 3.1 and 3.2, respectively, and thus conclude that this measure captures content similarity rather than stylistic similarity. 1998) for comparing texts. The construction of the semantic space was done using the evaluation corpora (see Section 3). We also used the vector space model Explicit Semantic Analysis (ESA) <ref type="bibr" target="#b18">(Gabrilovich and Markovitch, 2007)</ref>. Besides WordNet, we used two additional lexical-semantic resources for the construction of the ESA vector space: Wikipedia 4 and Wiktionary 5 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Structural Similarity</head><p>As discussed above, we presume that content similarity alone is not a reliable indicator of text reuse. Two independently written texts about the same topic are likely to make use of a common vocabulary to a certain extent. We thus propose to also use measures of structural similarity which compute similarity based on structural aspects inherent to the compared texts.</p><p>Stopword n-grams <ref type="bibr" target="#b52">(Stamatatos, 2011)</ref> are based on the idea that text reuse often preserves syntactic similarities while exchanging content words. Thus, the measure removes all content words while preserving only stopwords. All n-grams of both texts are then compared using the containment measure <ref type="bibr" target="#b6">(Broder, 1997)</ref>. We tested n-gram sizes for n = 2, 3, . . . , 15.</p><p>For the same reason, we also included part-of-speech n-grams in our feature set. Disregarding the actual words that appear in two given texts, computing n-grams along part-of-speech tags allows to detect syntactic similarities between these texts. Again, we tested n-gram sizes for n = 2, 3, . . . , 15, and compared the two sets using the containment measure <ref type="bibr" target="#b6">(Broder, 1997)</ref>.</p><p>We also employed two similarity measures between pairs of words <ref type="bibr" target="#b24">(Hatzivassiloglou et al., 1999)</ref>. The word pair order measure assumes that a similar syntactical structure in reused texts may cause two words to occur in the same order in both texts (with any number of words in between). The complementary word pair distance measure counts the number of words which lie between those of a given pair. For each measure, we computed feature vectors for both texts along all shared word pairs and compared the vectors using Pearson's correlation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Stylistic Similarity</head><p>Measures of stylistic similarity adopt ideas from authorship attribution <ref type="bibr" target="#b44">(Mosteller and Wallace, 1964)</ref> or use statistical properties of texts to compute text similarity. The type-token ratio (TTR) <ref type="bibr" target="#b53">(Templin, 1957)</ref>, for example, compares the vocabulary richness of two texts. However, it suffers from sensitivity to variations in text length and the assumption of textual homogeneity (McCarthy and Jarvis, 2010): As a text gets longer, the increase of tokens is linear, while the increase of types steadily slows down. In consequence, lexical repetition causes the TTR value to vary, while it does not necessarily entail that a reader perceives changes in the vocabulary usage. Secondly, textual homogeneity is the assumption of the existence of a single lexical diversity level across a whole text, which may be violated by different rhetorical strategies. Sequential TTR (McCarthy and Jarvis, 2010) alleviates these shortcomings. It iteratively computes a TTR score for a dynamically growing text segment until a point of saturation -i.e. a fixed TTR score of .72 -is reached, then starts anew from that position in the text for a new segment. The final lexical diversity score is computed as the number of tokens divided by the number of segments.</p><p>Inspired by <ref type="bibr" target="#b57">Yule (1939)</ref> who discussed sentence length as a characteristic of style, we also used two simple measures, sentence length and token length, in our system. These measures compute the average number of tokens per sentence and the average number of characters per token.  Additionally, we compared the average sentence and token lengths between the reused text and the original source. We refer to these measures as sentence ratio and token ratio, respectively.</p><p>Finally, we compare texts by their function word frequencies <ref type="bibr" target="#b15">(Dinu and Popescu, 2009)</ref> which have shown to be good style indicators in authorship attribution studies. Following the original work, this measure uses a set of 70 function words identified by <ref type="bibr" target="#b44">Mosteller and Wallace (1964)</ref> and computes feature vectors of their frequencies for each possibly reused document and the source text. The comparison of the vectors is then performed using Pearson's correlation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments &amp; Results</head><p>We utilized three datasets for the evaluation of our system which originate in the fields of plagiarism detection, journalistic text reuse detection, and paraphrase recognition: the Wikipedia Rewrite Corpus <ref type="bibr" target="#b14">(Clough and Stevenson, 2011)</ref>, the METER Corpus <ref type="bibr" target="#b19">(Gaizauskas et al., 2001)</ref>, and the Webis Crowd Paraphrase Corpus <ref type="bibr" target="#b8">(Burrows et al., 2012)</ref>, described below.</p><p>We carried out the same evaluation procedure for each of the three datasets: First, we computed text similarity scores between all pairs of possibly reused texts and their original sources using all the measures introduced in Section 2. We then used these scores as features for two machine learning classifiers in order to combine them across the three dimensions content, structure, and style. We experimented with two classifiers from the WEKA toolkit <ref type="bibr" target="#b23">(Hall et al., 2009)</ref>: a Naive Bayes classifier and a C4.5 decision tree classifier (J48 implementation).</p><p>In a 10-fold cross-validation setup, we ran three sets of experiments as follows: (i) First, we tested only the text similarity scores of one single measure at a time as single feature for the classifiers, in order to determine the individually best-performing measures per similarity   dimension. (ii) We then combined the measures per dimension by using multiple text similarity scores as feature set, in order to determine the performance of multiple measures within a single dimension. (iii) Finally, we combined the measures across dimensions to determine the best overall configuration. We compare our results with two baselines: the majority class baseline and the word trigram similarity measure Ferret <ref type="bibr" target="#b38">(Lyon et al., 2004)</ref> (see Section 2.1). Additionally, we report the best results from the literature for comparison.</p><p>Evaluation was carried out in terms of accuracy andF 1 score. By accuracy, we refer to the number of correctly predicted texts divided by the total number of texts. As the class distributions in both datasets are skewed, we report the overallF 1 score as the arithmetic mean across the F 1 scores of all classes in order to account for the class imbalance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Wikipedia Rewrite Corpus</head><p>Dataset The dataset contains 100 pairs of short texts (193 words on average). For each of 5 questions about topics of computer science (e.g. "What is dynamic programming?"), a reference answer (source text, henceforth) has been manually created by copying portions of text from a suitable Wikipedia article. Text reuse now occurs between a source text and an answer given by one of 19 participants. The participants were asked to provide short answers, each of which should comply to one of 4 rewrite levels and hence reuse the source text to a varying extent.</p><p>According to the degree of rewrite, the dataset is 4-way classified as cut &amp; paste (38 texts; simple copy of text portions from the Wikipedia article), light revision (19; synonym substitutions and changes of grammatical structure allowed), heavy revision (19; rephrasing of Wikipedia excerpts using different words and structure), and no plagiarism (19; answer written independently from the Wikipedia article). An example of a heavy revision was given in Figure <ref type="figure">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>We summarize the results on this dataset in Table <ref type="table" target="#tab_3">2</ref>. <ref type="bibr">8</ref> In the best configuration, when combining similarity measures across dimensions, our system achieves a performance of   <ref type="bibr" target="#b14">Clough and Stevenson (2011)</ref> by 5.4% points in terms ofF 1 score compared to their reported numbers, and by 15.3% points compared to our re-implementation of this system 7 . Their system uses a Naive Bayes classifier with only a very small feature set: word n-gram containment (n = 1, 2, . . . , 5) and longest common subsequence. For comparison, we re-implemented their system and also applied it to the two datasets in the remainder of this paper. We report our findings in Sections 3.2 and 3.3.</p><p>In Table <ref type="table" target="#tab_1">1</ref>, we further report the detailed results for a selected set of individual text similarity measures, listed by similarity dimension. 9 Due to space limitations, we only report a selected set of best-performing measures per dimension and compare them with the baselines: While the majority class baseline performs very poor on this dataset (F 1 = .143), the Ferret baseline achievesF 1 = .517. Some content similarity measures such as word 2-grams containment show a reasonable performance (F 1 = .683), while structural measures cannot exceedF 1 = .554, and stylistic measures perform only slightly better than the majority class baseline (F 1 = .296).</p><p>In Table <ref type="table" target="#tab_5">3</ref>, we report the best results for the combinations of text similarity measures within and across dimensions. When we combine the measures within their respective dimensions, content outperforms structural and stylistic similarity. However, all combinations of measures across dimensions in addition to content similarity improve the results. The best performance is achieved by combining the three similarity measures longest common subsequence, stopword 10-grams, and character 5-gram profiles from the two dimensions content and structure. This supports our hypothesis that the similarity computation process indeed profits from dimensions other than content. The effects of dimension combination held true regardless of the classifier used, even though the decision tree classifier performed consistently better than Naive Bayes.</p><p>Error Analysis We present the confusion matrix for our best configuration in Table <ref type="table" target="#tab_3">2</ref>. In total, 15 texts out of 95 have been classified with the wrong label. While all texts except a single one in the class no plagiarism have been classified correctly, 67% of errors (10 texts) are due to misclassifications in the light and heavy revision classes. We assume that these errors are due to questionable gold standard annotations as the annotation guidelines for these two classes are highly similar <ref type="bibr" target="#b14">(Clough and Stevenson, 2011)</ref>. For the light revision class, the annotators "could alter the text in some basic ways", thereby "altering the grammatical structure (i.e. paraphrasing)." Likewise, for the heavy revision class, the annotation manual expected the   As each text of this dataset was written by only a single person for a given rewrite category, we decided to conduct an annotation study, in which we were mostly interested in the inter-rater agreement of the subjects. We asked 3 participants to rate the degree of text reuse and provided them with the original annotation guidelines. We used a generalization of <ref type="bibr" target="#b49">Scott's (1955)</ref> π-measure for calculating a chance-corrected inter-rater agreement for multiple raters, which is known as Fleiss' (1971) κ and Carletta's (1996) K. 10 In summary, the results 11 of our study support our hypothesis that the annotators mostly disagree for the light and heavy revision classes, with fair 12 agreements of κ = .34 and κ = .28, respectively. For the cut &amp; paste and no plagiarism classes, we observe moderate 12 agreements, κ = .53 and κ = .56, respectively.</p><p>Based on these insights, we decided to fold the light and heavy revision classes into a single class potential plagiarism. This approach was also briefly discussed by <ref type="bibr" target="#b14">Clough and Stevenson (2011)</ref>, though not carried out in their work. We report the corresponding results and the confusion matrix in Table <ref type="table" target="#tab_7">4</ref>. As the classification task gets easier by the reduction to three classes, the results for the Ferret baseline improve, fromF 1 = .517 toF 1 = .745. The re-implementation of the system by <ref type="bibr" target="#b14">Clough and Stevenson (2011)</ref> achievesF 1 = .788. Our system again outperforms all other systems withF 1 = .859.</p><p>In our envisioned semi-supervised application scenario, potentially reused texts are presented to users in an informative manner. Here, fine-grained distinctions are not necessary, and we decided to go even one step further and fold all potential cases of text reuse. This variant of the dataset results in a binary classification of plagiarized/non-plagiarized texts. We present An exhaustive discussion of inter-rater agreement measures is given by <ref type="bibr" target="#b2">Artstein and Poesio (2008</ref>   <ref type="table" target="#tab_8">5</ref>. In this simplified setting, even the Ferret baseline achieves an excellent performance ofF 1 = .935. Our approach still slightly outperforms (F 1 = .967) the re-implementation of the system by <ref type="bibr" target="#b14">Clough and Stevenson (2011)</ref>.</p><p>An interesting observation across all three variants of the dataset is that the same three texts always constitute severe error instances where e.g. a cut &amp; paste text is falsely labeled as no plagiarism, which is more severe than mislabeling a light revision as a heavy revision. Two of the three cases account for the texts which describe the PageRank algorithm. One of these instances was falsely labeled as cut &amp; paste while it is non-plagiarized, and the other one vice-versa. We attribute the misclassifications to the model built up in the classifier's training phase.</p><p>In the envisioned semi-supervised setting, the remaining less severe error instances, where e.g. a light revision was classified as a heavy revision, can be reviewed by a user of the system. We suppose it is even hard for users to draw a strict line between possibly reused and non-reused texts, as this heavily depends on external effects such as user intentions and the task at hand.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">METER Corpus</head><p>Dataset The dataset contains news sources from the UK Press Association (PA) and newspaper articles from 9 British newspapers that reused the PA source texts to generate their own texts. The complete dataset contains 1,716 texts from two domains: law &amp; court and show business. All newspaper articles have been annotated whether they are wholly derived from the PA sources (i.e. the PA text has been used exclusively as text reuse source), partially derived (the PA text has been used in addition to other sources), or non-derived (the PA text has not been used at all).</p><p>Several newspaper texts, though, have more than a single PA source in the original dataset where it is unclear which (if not all) of the source stories have been used to generate the rewritten story. However, for text reuse detection it is important to have aligned pairs of reused texts and source texts. Therefore, we followed Sánchez-Vega et al. ( <ref type="formula">2010</ref>) and selected a subset of texts where only a single source story is present in the dataset. This leaves 253 pairs of short texts (205 words on average). We further followed <ref type="bibr" target="#b48">Sánchez-Vega et al. (2010)</ref> and folded the annotations to a binary classification of 181 reused (wholly/partially derived) and 72 non-reused instances in order to carry out a comparable evaluation study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>We summarize the results on this dataset in Table <ref type="table" target="#tab_10">6</ref>. In the best configuration, our system achieves an overall performance ofF 1 = .768. It outperforms the best reference system by Sánchez-Vega et al. (2010) by 6.3% points in terms ofF 1 score. Their system uses a Naive Bayes classifier with two custom features which compare texts based on the length and frequency of common word sequences and the relevance of individual words. As in Section 3.1, we further report the detailed results for a selected set of individual text similarity measures   <ref type="table" target="#tab_1">1</ref>. From these figures, we learn that many text similarity measures cannot exceed the simple majority class baseline (F 1 = .417) when applied individually.</p><p>In Table <ref type="table" target="#tab_12">7</ref>, we show that the performance of text reuse detection always improves over individual measures (cf. Table <ref type="table" target="#tab_1">1</ref>) when we combine the measures within their respective dimensions. An exception is the combination of structural similarity measures, which only performs on the same level as the best individual measure part-of-speech 3-grams containment. Combinations of content similarity measures show a better performance than combinations of structural or stylistic measures. Our system achieves its best performance on this dataset when text similarity measures are combined across all three dimensions content, structure, and style. The best configuration resulted from using a Naive Bayes classifier with the following measures: Greedy String Tiling, stopword 12-grams, and Sequential TTR. As for the previous dataset, the effects of dimension combination held true regardless of the classifier used.</p><p>The influence of the stylistic similarity measures is particularly interesting to note. In contrast to the Wikipedia Rewrite Corpus, including these measures in the composition improves the results on this dataset: Our classifier is able to detect similarity even for reused texts by expert journalists. This is due to the fact that a journalistic text which reuses the original press agency source most likely also shows stylistic similarity in terms of e.g. vocabulary richness.</p><p>Error Analysis We present the confusion matrix for our best configuration in Table <ref type="table" target="#tab_10">6</ref>. In total, 50 texts out of 253 have been classified incorrectly: 30 instances of text reuse have not been identified by the classifier, and 20 non-reused texts have been mistakenly labeled as such. However, the original annotations have been carried out by only a single annotator <ref type="bibr" target="#b19">(Gaizauskas et al., 2001)</ref> which may have resulted in subjective judgments. Thus, as for the previous dataset in Section 3.1, we conducted an annotation study with three annotators to gain further insights into the data. The results <ref type="bibr">11</ref> show that for 61% of all texts the annotators fully agree. The chance-corrected Fleiss' (1971) agreement κ = .47 is moderate 12 .</p><p>For the 30 instances of text reuse which have not been identified by the classifier, it is particularly interesting to note that many errors are due to the fact that a lower overall text similarity between the possibly reused text and the original source does not necessarily entail the label no reuse. The newspaper article about the English singer-songwriter Liam Gallagher, for example, is originally labeled as text reuse. However, our classifier falsely assigned the label no reuse.  We conclude that applications will benefit from an improved classifier which better deals with theses instances. For example, similarity features could be computed per section, not per document, which would allow to also identify potential instances of text reuse for only partially matching texts. The currently achieved performance (see Table <ref type="table" target="#tab_10">6</ref>) of text reuse detection, though, is sufficient for our envisioned semi-supervised application scenario where content authors are provided only with suggestions of potential instances of text reuse and then are free to decide how to proceed, e.g. to merge both texts. The final decision probably also depends on external factors such as user intentions and the task at hand.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Webis Crowd Paraphrase Corpus</head><p>Dataset The dataset was originally introduced as part of the PAN 2010 international plagiarism detection competition <ref type="bibr" target="#b45">(Potthast et al., 2010)</ref>. It contains 7,859 pairs of original texts along with their paraphrases (28 to 954 words in length) with 4,067 (52%) positive and 3,792 (48%) negative samples. The original texts are book excerpts from Project Gutenberg 14 , and the corresponding paraphrases were acquired in a crowdsourcing process using Amazon Mechanical Turk <ref type="bibr" target="#b9">(Callison-Burch and Dredze, 2010)</ref>. In the manual filtering process 15 of all acquired paraphrases, <ref type="bibr" target="#b8">Burrows et al. (2012)</ref> hereby follow the paraphrase definition by <ref type="bibr" target="#b5">Boonthum (2004)</ref>, where a good paraphrase exhibits patterns such as synonym use, changes between active and passive voice, or changing word forms and parts of speech, and a bad paraphrase is rather e.g. a (near-)duplicate or an automated one-for-one word substitution. This definition implies that a more sophisticated interpretation of text similarity scores needs to be learned, where e.g. (near-)duplicates with very high similarity scores are in fact negative samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>We summarize the results on this dataset in Table <ref type="table" target="#tab_14">8</ref>. Even though the Ferret baseline is a strong competitor (F 1 = .789), our approach achieves the best results on this dataset with F 1 = .852. The results reported by <ref type="bibr" target="#b8">Burrows et al. (2012)</ref> are slightly worse (F 1 = .837). Their best score was achieved by using a k-nearest neighbor classifier with a feature set of 10 similarity measures. They exclusively used similarity measures that operate on the texts' string sequences and thus capture the content dimension of text similarity only, e.g. <ref type="bibr" target="#b36">Levenshtein (1966)</ref> distance and a word n-gram similarity measure. As in the previous sections, we report the detailed results for a selected set of individual text similarity measures in  As for the previous datasets, our hypothesis holds true that the combination of similarity dimensions improves the results: When we combine the similarity features within each of the respective dimensions, the performance numbers increase (see Table <ref type="table" target="#tab_16">9</ref> as compared to Table <ref type="table" target="#tab_1">1</ref>). The combination of content similarity measures is stronger than the combination of structural and stylistic similarity measures, and performs on the same level as the original results reported by <ref type="bibr" target="#b8">Burrows et al. (2012)</ref>. This is to be expected, as their system uses a feature set which also addresses the content dimension exclusively.</p><p>When we combine measures across dimensions, the results improve even further. An exception is the combination of content and structural measures, which performs slightly worse than content measures alone due to the lower performance of structural measures on this dataset. The best configuration of our system resulted from combining all three dimensions content, structure, and style in a single classification model using the decision tree classifier, resulting in F 1 = .852. The final feature set contains 16 text similarity features which are listed in Table <ref type="table" target="#tab_1">10</ref>.</p><p>Error Analysis We present the confusion matrix for our best classification in Table <ref type="table" target="#tab_14">8</ref>. In total, 1,172 (15%) out of 7,859 text pairs have been classified incorrectly. Out of these, our classifier mistakenly labeled 759 instances of negative samples as true paraphrases, while 413 cases of true paraphrases were not recognized. However, in our opinion the 759 false positives are less severe errors in our envisioned semi-supervised application setting, as user intentions and the current task at hand may highly influence a user's decision to consider texts as reused or not.</p><p>In general, we attribute the errors to the particular properties of this dataset, which differ from those of the Wikipedia Rewrite Corpus and the METER Corpus (see Sections 3.1 and 3.2). For those two datasets, the more similar two texts are, the higher their degree of text reuse. For the Webis Crowd Paraphrase Corpus, however, a different interpretation needs to be learned by the classifier: Here, (near-)duplicates and texts with automated word-by-word substitutions, which will receive high similarity scores by any of our content similarity measures, are in fact annotated as bad paraphrases, i.e. negative samples. Unrelated texts, empty samples, or texts alike also belong to the class of negative samples. In consequence, positive samples are only those in the medium similarity range. We assume that the more elaborate definition of positive and negative cases makes it more difficult to learn a proper model for the given data.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions and Future Work</head><p>The motivation for this work stemmed from the hypothesis that content features alone are not a reliable indicator for text reuse detection. As illustrated in Figure <ref type="figure">1</ref>, a reused text may also contain modifications such as split sentences, changed order of reused parts, or stylistic variance. We thus devised an architecture which composes diverse text similarity measures in a supervised classification model. In this model, we overcome the traditional limitation of text similarity measures to content features and compute similarity along three characteristic dimensions inherent to texts: content, structure, and style.</p><p>We evaluated our classification model on three standard datasets where text reuse is prevalent and which originate in the fields of plagiarism detection, journalistic text reuse detection, and paraphrase recognition: the Wikipedia Rewrite Corpus <ref type="bibr" target="#b14">(Clough and Stevenson, 2011)</ref>, the METER Corpus <ref type="bibr" target="#b19">(Gaizauskas et al., 2001)</ref>, and the Webis Crowd Paraphrase Corpus <ref type="bibr" target="#b8">(Burrows et al., 2012)</ref>. Based on the evaluation results, we discussed the influence of each of the similarity dimensions, and demonstrated empirically that text reuse can be best detected if measures are combined across dimensions, so that a wide variety of text features are taken into consideration. The composition consistently outperforms previous approaches across all datasets.</p><p>As we showed, similarity computation works best if the similarity dimensions are chosen well with respect to the type of text reuse at hand. For the Wikipedia Rewrite Corpus, for example, the stylistic similarity features perform only poorly, which is why the composition of all three dimensions performs slightly worse than than the combination of only content and structural features. For the other two datasets, however, stylistic similarity is a strong dimension within the composition, and consequently the best performance is reached when combining all three dimensions. Based on these insights, we conclude that for novel datasets it is essential to address the dimensions explicitly in the annotation process, so that text reuse detection approaches can be evaluated precisely against particular characteristics of different kinds of data.</p><p>For future work, we expect that considering a dimensional representation of text similarity features will also benefit any other task where text similarity computation is fundamental and which is yet limited to content features, e.g. paraphrase recognition or automatic essay grading.</p><p>For the latter, we see great potential for improvements by including, for example, measures for grammar analysis, lexical complexity, or measures assessing text organization with respect to the discourse elements. However, each task exhibits particular characteristics which influence the choice of a suitable set of similarity dimensions. As discussed above, a particular dimension may or may not contribute to an overall improvement based on the nature of the data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Content</head><label></label><figDesc>ESA (WordNet, with + w/o stopwords), Greedy String Tiling, Jaro, Longest Common Substring, Longest Common Subseq. (2 norm.), n-gram Jaccard (n = {6, 14, 15}), Resnik (SMT wrapper)StructureLemma Pair Ordering, POS 2-grams Jaccard, Stopword 6-grams Style Function Word Frequencies, Sequential TTR, Token Ratio</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Performance of selected similarity measures on the Wikipedia Rewrite Corpus, the METER Corpus, and the Webis Crowd Paraphrase Corpus, grouped by similarity dimension</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Results and confusion matrix (expected class vs. classification result) for the best classification on the Wikipedia Rewrite Corpus for the original 4-way classification</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Results of the best combinations of text similarity measures within and across dimensions on the Wikipedia Rewrite Corpus F</figDesc><table /><note>1 = .811. It outperforms the best reference system by</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Results and confusion matrix on the Wikipedia Rewrite Corpus for the folded 3-way classification</figDesc><table><row><cell>System</cell><cell cols="2">Acc.F 1</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Majority Class Baseline Ferret Baseline Clough and Stevenson (2011)</cell><cell>.600 .937</cell><cell>.375 .935</cell><cell>exp. plagiarism class.</cell><cell>plagiarism 55</cell><cell>no plag. 2</cell></row><row><cell>-our re-implementation -as reported</cell><cell>.958 .947</cell><cell>.957 n/a</cell><cell>no plag.</cell><cell>1</cell><cell>37</cell></row><row><cell>Our Approach</cell><cell>.968</cell><cell>.967</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table /><note>Results and confusion matrix on the Wikipedia Rewrite Corpus for the folded binary classification annotators to "rephrase the text to generate an answer with the same meaning as the source text, but expressed using different words and structure."</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>http://www.ukp.tu-darmstadt.de/data/text-similarity/text-reuse-annotations</figDesc><table><row><cell>System</cell><cell cols="2">Acc.F 1</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Majority Class Baseline Ferret Baseline Clough and Stevenson (2011) 13 Sánchez-Vega et al. (2010)</cell><cell>.715 .684 .692 .783</cell><cell>.417 .535 .680 .705</cell><cell>class. reuse exp. no reuse</cell><cell>reuse 151 20</cell><cell>no reuse 30 52</cell></row><row><cell>Our Approach</cell><cell>.802</cell><cell>.768</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>).</cell></row></table><note>11  </note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Results and confusion matrix for the best classification on the METER Corpus the results and the corresponding confusion matrix in Table</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 :</head><label>7</label><figDesc>Results of the best combinations of text similarity measures within and across dimensions on the METER Corpus in Table</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>It turns out, though, that the reused text is about four times as long as the original press agency source, with lots of new facts being introduced there. Consequently, only a low similarity score</figDesc><table><row><cell>System</cell><cell cols="2">Acc.F 1</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Majority Class Baseline Ferret Baseline Clough and Stevenson (2011) 13 Burrows et al. (2012)</cell><cell>.517 .794 .798 .839</cell><cell>.341 .789 .795 .837</cell><cell>class. paraphrase exp. no para.</cell><cell>paraphrase 3,654 759</cell><cell>no para. 413 3,033</cell></row><row><cell>Our Approach</cell><cell>.853</cell><cell>.852</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 8 :</head><label>8</label><figDesc>Results and confusion matrix for the best classification on the Webis Crowd Paraphrase Corpus can be computed between the additional material in the newspaper article and the original source, and the overall similarity score decreases.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 1 .</head><label>1</label><figDesc>These figures show that www.gutenberg.org</figDesc><table><row><cell>Text Similarity Dimension</cell><cell cols="2">Acc.F 1</cell></row><row><cell>Combinations within dimensions</cell><cell></cell><cell></cell></row><row><cell>Content</cell><cell>.840</cell><cell>.839</cell></row><row><cell>Structure</cell><cell>.816</cell><cell>.814</cell></row><row><cell>Style</cell><cell>.819</cell><cell>.817</cell></row><row><cell>Combinations across dimensions</cell><cell></cell><cell></cell></row><row><cell>Content + Style Content + Structure Structure + Style Content + Structure + Style</cell><cell>.844 .838 .831 .853</cell><cell>.843 .838 .830 .852</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 9 :</head><label>9</label><figDesc>Results of the best combinations of text similarity measures within and across dimensions on the Webis Crowd Paraphrase Corpus regardless of the similarity dimension many measures achieve a very reasonable performance when applied individually, with the measures Greedy String Tiling and word 2-grams containment performing best.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 10 :</head><label>10</label><figDesc>Feature set used to achieve the best results on the Webis Crowd Paraphrase Corpus</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">In addition, we release an open-source framework which contains implementations of all discussed measures in order to stimulate the development of novel measures: http://code.google.com/p/dkpro-similarity-asl</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">www.wikipedia.org 5 www.wiktionary.org</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6"><ref type="bibr" target="#b12">Chong et al. (2010)</ref> reportF 1 = .698 in their original work. This figure, however, reflects the weighted arithmetic mean over all four classes of the dataset where one class is twice as prominent as each of the others. As dicussed in Section 3, we report allF 1 scores as the unweighted arithmetic mean in order to account for the class imbalance.7 While we were able to reproduce the results of the Ferret baseline as reported by<ref type="bibr" target="#b12">Chong et al. (2010)</ref>, our reimplementation of the system by Clough and Stevenson (2011) (Naive Bayes classifier, same feature set) resulted in a much lower overall performance. We observed the largest difference for the longest common subsequence measure, even though we used a standard implementation<ref type="bibr" target="#b1">(Allison and Dix, 1986)</ref> and normalized as described byClough and  Stevenson (2011).  8  Figures in italics are taken from the literature, while we (re-)implemented the remaining systems. This applies to all result tables in this paper.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">Table1also lists the detailed results for the METER Corpus and the Webis Crowd Paraphrase Corpus. We will discuss the numbers in the corresponding Sections 3.2 and 3.3.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12">Strength of agreement for κ values according to<ref type="bibr" target="#b33">Landis and Koch (1977)</ref> 13  We report the results for our re-implementation of the system by<ref type="bibr" target="#b14">Clough and Stevenson (2011)</ref>. In their original work, they did not evaluate on this dataset.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15"><ref type="bibr" target="#b8">Burrows et al. (2012)</ref> do not report any inter-annotator agreements for the filtering process, as the task was split across two annotators and each text pair was labeled by only a single annotator.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work has been supported by the Volkswagen Foundation as part of the Lichtenberg-Professorship Program under grant No. I/82806, and by the Klaus Tschira Foundation under project No. 00.133.2008. We thank Chris Biemann for his inspirations, as well as Carolin Deeg, Andriy Nadolskyy, and Artem Vovk for their participation in the annotation studies.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Detecting the Origin of Text Segments Efficiently</title>
		<author>
			<persName><forename type="first">O</forename><surname>Abdel-Hamid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Behzadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Christoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Henzinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Conference on World Wide Web</title>
				<meeting>the 18th International Conference on World Wide Web<address><addrLine>Madrid, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="61" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A bit-string longest-common-subsequence algorithm</title>
		<author>
			<persName><forename type="first">L</forename><surname>Allison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">I</forename><surname>Dix</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="305" to="310" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Inter-Coder Agreement for Computational Linguistics</title>
		<author>
			<persName><forename type="first">R</forename><surname>Artstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Poesio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="555" to="596" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A Reflective View on Text Similarity</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bär</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zesch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Recent Advances in Natural Language Processing</title>
				<meeting>the International Conference on Recent Advances in Natural Language Processing<address><addrLine>Hissar, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="515" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Plagiarism Detection across Distant Language Pairs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Labaka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics</title>
				<meeting>the 23rd International Conference on Computational Linguistics<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="37" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">iSTART: Paraphrase Recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Boonthum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Meeting of the Association for Computational Linguistics: Student Research Workshop</title>
				<meeting>the 42nd Meeting of the Association for Computational Linguistics: Student Research Workshop<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="31" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">On the resemblance and containment of documents</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Z</forename><surname>Broder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Compression and Complexity of Sequences</title>
				<meeting>Compression and Complexity of Sequences</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Syntactic clustering of the Web</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Z</forename><surname>Broder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Glassman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Manasse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International World Wide Web Conference</title>
				<meeting>the 6th International World Wide Web Conference<address><addrLine>Santa Clara, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="1157" to="1166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Paraphrase Acquisition via Crowdsourcing and Machine Learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Burrows</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="2012-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Creating Speech and Language Data With Amazon&apos;s Mechanical Turk</title>
		<author>
			<persName><forename type="first">C</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL HLT Workshop on Creating Speech and Language Data with Amazon&apos;s Mechanical Turk</title>
				<meeting>the NAACL HLT Workshop on Creating Speech and Language Data with Amazon&apos;s Mechanical Turk<address><addrLine>Los Angeles, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Assessing Agreement on Classification Tasks: The Kappa Statistic</title>
		<author>
			<persName><forename type="first">J</forename><surname>Carletta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="249" to="254" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Similarity Estimation Techniques from Rounding Algorithms</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Charikar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th Annual Symposium on Theory of Computing</title>
				<meeting>the 34th Annual Symposium on Theory of Computing<address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="380" to="388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Using Natural Language Processing for Automatic Detection of Plagiarism</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mitkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Plagiarism Conference</title>
				<meeting>the 4th International Plagiarism Conference<address><addrLine>Newcastle upon Tyne, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">METER: MEasuring TExt Reuse</title>
		<author>
			<persName><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gaizauskas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wilks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 40th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="152" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Developing a Corpus of Plagiarised Short Answers. Language Resources and Evaluation: Special Issue on Plagiarism and Authorship Analysis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stevenson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="5" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Ordinal measures in authorship identification</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd PAN Workshop. Uncovering Plagiarism, Authorship and Social Software Misuse</title>
				<meeting>the 3rd PAN Workshop. Uncovering Plagiarism, Authorship and Social Software Misuse<address><addrLine>San Sebastian, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="62" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">WordNet: An Electronic Lexical Database</title>
		<author>
			<persName><forename type="first">C</forename><surname>Fellbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Measuring nominal scale agreement among many raters</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Fleiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="378" to="382" />
			<date type="published" when="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Computing Semantic Relatedness using Wikipediabased Explicit Semantic Analysis</title>
		<author>
			<persName><forename type="first">E</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Markovitch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Joint Conference on Artificial Intelligence</title>
				<meeting>the 20th International Joint Conference on Artificial Intelligence<address><addrLine>Hyderabad, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1606" to="1611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The METER Corpus: A corpus for analysing journalistic text reuse</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gaizauskas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wilks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Arundel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Piao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Corpus Linguistics 2001 Conference</title>
				<meeting>the Corpus Linguistics 2001 Conference</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Conceptual Spaces: The Geometry of Thought</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gärdenfors</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Seven strictures on similarity</title>
		<author>
			<persName><forename type="first">N</forename><surname>Goodman</surname></persName>
		</author>
		<editor>Goodman, N.,</editor>
		<imprint>
			<date type="published" when="1972" />
			<biblScope unit="page" from="437" to="446" />
			<pubPlace>Bobbs-Merrill</pubPlace>
		</imprint>
	</monogr>
	<note>editor, Problems and projects</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Gusfield</surname></persName>
		</author>
		<title level="m">Algorithms on Strings, Trees and Sequences: Computer Science and Computational Biology</title>
				<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Reutemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
		<title level="m">The WEKA Data Mining Software: An Update. SIGKDD Explorations</title>
				<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Detecting text similarity over short passages: Exploring linguistic feature combinations via machine learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Hatzivassiloglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Klavans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Eskin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora</title>
				<meeting>the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora<address><addrLine>College Park, MD, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="203" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Finding Near-Duplicate Web Pages: A Large-Scale Evaluation of Algorithms</title>
		<author>
			<persName><forename type="first">M</forename><surname>Henzinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
				<meeting>the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="284" to="291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Methods for identifying versioned and plagiarized documents</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Hoad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society of Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="203" to="215" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Advances in record linkage methodology as applied to the 1985 census of Tampa Florida</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Jaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="issue">406</biblScope>
			<biblScope unit="page" from="414" to="420" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Semantic similarity based on corpus statistics and lexical taxonomy</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Conrath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on Research in Computational Linguistics</title>
				<meeting>the 10th International Conference on Research in Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">N-gram-based author profiles for authorship attribution</title>
		<author>
			<persName><forename type="first">V</forename><surname>Keselj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cercone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference of the Pacific Association for Computational Linguistics</title>
				<meeting>the Conference of the Pacific Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="255" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Europarl: A Parallel Corpus for Statistical Machine Translation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th Machine Translation Summit</title>
				<meeting>the 10th Machine Translation Summit<address><addrLine>Phuket Island, Thailand</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Moses: Open Source Toolkit for Statistical Machine Translation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Herbst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions</title>
				<meeting>the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume the Demo and Poster Sessions<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">An introduction to latent semantic analysis</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Foltz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Laham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Discourse Processes</title>
				<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="259" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The measurement of observer agreement for categorical data</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Landis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">G</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="159" to="174" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A Computational Model of Text Reuse in Ancient Literary Texts</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics</title>
				<meeting>the 45th Annual Meeting of the Association of Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="472" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Leuf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cunningham</surname></persName>
		</author>
		<title level="m">The Wiki Way: Collaboration and Sharing on the Internet</title>
				<imprint>
			<publisher>Addison-Wesley Professional</publisher>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Binary codes capable of correcting deletions, insertions, and reversals</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">I</forename><surname>Levenshtein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Soviet Physics Doklady</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="707" to="710" />
			<date type="published" when="1966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">An information-theoretic definition of similarity</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning</title>
				<meeting>International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="296" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A theoretical basis to the automated detection of copying between texts, and its practical implementation in the Ferret plagiarism and collusion detector</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malcolm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Plagiarism: Prevention, Practice and Policies Conference</title>
				<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Detecting short passages of similar text in large document collections</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malcolm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dickerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="118" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Detecting Near-Duplicates for Web Crawling</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Manku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Sarma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International World Wide Web Conference</title>
				<meeting>the 16th International World Wide Web Conference<address><addrLine>Banff, AB, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="141" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">MTLD, vocd-D, and HD-D: A validation study of sophisticated approaches to lexical diversity assessment</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Mccarthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jarvis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Behavior research methods</title>
				<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="381" to="392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Corpus-based and Knowledge-based Measures of Text Semantic Similarity</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Corley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Strapparava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st National Conference on Artificial Intelligence</title>
				<meeting>the 21st National Conference on Artificial Intelligence<address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="775" to="780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">An efficient domain-independent algorithm for detecting approximately duplicate database records</title>
		<author>
			<persName><forename type="first">A</forename><surname>Monge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Elkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGMOD Workshop on Data Mining and Knowledge Discovery</title>
				<meeting>the SIGMOD Workshop on Data Mining and Knowledge Discovery<address><addrLine>Tucson, AZ, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="23" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Inference and disputed authorship: The Federalist</title>
		<author>
			<persName><forename type="first">F</forename><surname>Mosteller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wallace</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1964" />
			<publisher>Addison-Wesley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Overview of the 2nd International Competition on Plagiarism Detection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Eiselt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Notebook Papers of CLEF 10 Labs and Workshops</title>
				<meeting><address><addrLine>Padua, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Using Information Content to Evaluate Semantic Similarity in a Taxonomy</title>
		<author>
			<persName><forename type="first">P</forename><surname>Resnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Joint Conference on Artificial Intelligence</title>
				<meeting>the 14th International Joint Conference on Artificial Intelligence<address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="448" to="453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Introduction to Modern Information Retrieval</title>
		<author>
			<persName><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Mcgill</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983" />
			<publisher>McGraw-Hill</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Towards Document Plagiarism Detection Based on the Relevance and Fragmentation of the Reused Text</title>
		<author>
			<persName><forename type="first">F</forename><surname>Sánchez-Vega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Villaseñor-Pineda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Montes-Y-Gómez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th Mexican International Conference on Artificial Intelligence</title>
				<meeting>the 9th Mexican International Conference on Artificial Intelligence<address><addrLine>Pachuca, Mexico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="24" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Reliability of content analysis: The case of nominal scale coding</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Public Opinion Quarterly</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="321" to="325" />
			<date type="published" when="1955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Prediction and Entropy of Printed English</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Shannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell System Technical Journal</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="50" to="64" />
			<date type="published" when="1951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A statistical interpretation of term specificity and its application in retrieval</title>
		<author>
			<persName><forename type="first">Spärck</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Documentation</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="21" />
			<date type="published" when="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Plagiarism detection using stopword n-grams</title>
		<author>
			<persName><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2512" to="2527" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Certain language skills in children</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Templin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1957" />
			<publisher>University of Minnesota Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Features of Similarity</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tversky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Psychological Review</title>
				<imprint>
			<date type="published" when="1977" />
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page" from="327" to="352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">String Comparator Metrics and Enhanced Decision Rules in the Fellegi-Sunter Model of Record Linkage</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E</forename><surname>Winkler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Section on Survey Research Methods</title>
				<meeting>the Section on Survey Research Methods</meeting>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="page" from="354" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">YAP3: Improved detection of similarities in computer program and other texts</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Wise</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th SIGCSE technical symposium on Computer science education</title>
				<meeting>the 27th SIGCSE technical symposium on Computer science education<address><addrLine>Philadelphia, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="130" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">On sentence-length as a statistical characteristic of style in prose: With application to two cases of disputed authorship</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">U</forename><surname>Yule</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3/4</biblScope>
			<biblScope unit="page" from="363" to="390" />
			<date type="published" when="1939" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
